{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of linear signal processing in photon counting lidar\n",
    "This notebook was developed as a suppliment to the publication\n",
    "\n",
    "Matthew Hayman, Robert A. Stillwell and Scott M. Spuler, \n",
    "\"Optimization of linear photon counting lidar signal processing through Poisson thinning,\"\n",
    "submitted to Optics Letters *In Review*, DOI:TBD\n",
    "\n",
    "The notebook demonstrates the basic concepts for optimizing linear smoothing\n",
    "kernenls to a particular lidar scene by splitting an observed photon count\n",
    "profile into a \"fit\" and \"verification\" profile.  Smoothing kernels are applied\n",
    "to the \"fit\" profile then evaluated against the verification data set to find \n",
    "the optimal filter kernel.\n",
    "\n",
    "The code and functions contained in this notebook are available for public use\n",
    "so long as the original publication is referenced, but in the interest of independent verification, we strongly recommend that you write your own functions.  While every effort is made to provide accurate calcualtions, this code is not guaranteed to be free from errors.\n",
    "\n",
    "The data used in this example is from an NCAR MicroPulse DIAL (MPD) [1,2] using the potassium HSRL channels [3].  This \n",
    "data is provided for example purposes only and should not be used in scientific\n",
    "study.  No data quality assurance can be provided for these datasets.\n",
    "\n",
    "1. Spuler et al., \"Field-deployable diode-laser-based differential absorption lidar (DIAL) for profiling water vapor,\" Atmos. Meas. Tech., 8, 1073â€“1087, DOI:10.5194/amt-8-1073-2015 (2015).\n",
    "\n",
    "2. NCAR/EOL Remote Sensing Facility, \"NCAR MPD data. Version 1.0,\"Retrieved 9 Jan 2020, DOI:10.26023/MX0D-Z722-M406.\n",
    "\n",
    "3. Stillewell et al.,\"Demonstration of a combined differential absorption and high spectral resolution lidar for profiling atmospheric temperature,\" Opt. Express, 28, 71-93, DOI: 10.1364/OE.379804 (2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "fc80df9c-6bde-4141-8896-29bcc42292b9"
    }
   },
   "outputs": [],
   "source": [
    "# import python libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr  # not part of the standard python environemnt. if using anaconda: conda install xarray\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "94399d8d-a6a8-458e-9ba7-0c1462efd429"
    }
   },
   "outputs": [],
   "source": [
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "40194af8-85a4-4216-8510-ea17fbaa57b1"
    }
   },
   "outputs": [],
   "source": [
    "ncfile = 'mpd05.20181022T12300019921_20181022T15163019921.nc'\n",
    "# ncfile = 'mpd05.20181022T18032019921_20181022T20495019531.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of lidar profile names to be loaded\n",
    "lidar_profile_data = ['Combined_Counts','Molecular_Counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "0e595ff4-fda4-472b-9f61-514ce4a0c825"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function definitions for optimizing the filter\n",
    "\"\"\"\n",
    "\n",
    "def poisson_thin(pc_data):\n",
    "    \"\"\"\n",
    "    Poisson thin photon counting data\n",
    "    \n",
    "    inputs:\n",
    "        pc_data - array of raw photon count data\n",
    "            This is assumed to have a Poisson PDF\n",
    "    outputs:\n",
    "        pc_1,pc_2 - the resulting thinned photon count\n",
    "            arrays\n",
    "    \"\"\"\n",
    "    # the first thinned profile is calculated using a binomial random\n",
    "    # number generator to simulate \n",
    "    # flipping a coin to decide if a photon count is included in this\n",
    "    # profile or not\n",
    "    pc_1 = np.random.binomial(pc_data.astype(np.int),0.5,size=pc_data.shape)\n",
    "    # the second profile is whatever photon counts are left\n",
    "    pc_2 = pc_data-pc_1\n",
    "    \n",
    "    return pc_1,pc_2\n",
    "\n",
    "def build_Gaussian_kernel(sigt,sigz,norm=True):\n",
    "    \"\"\"\n",
    "    Generates a Gaussian convolution kernel for\n",
    "    standard deviations sigt and sigz in units of grid points.\n",
    "    sigt and sigz are defined in units of grid steps\n",
    "    \"\"\"        \n",
    "\n",
    "    \n",
    "    nt = np.round(4*sigt) # estimate size of time grid needed\n",
    "    nz = np.round(4*sigz) # estimate size of range grid needed\n",
    "    t = np.arange(-nt,nt+1) # create time grid     \n",
    "    z = np.arange(-nz,nz+1) # create range grid\n",
    "    \n",
    "    \n",
    "    # build Gaussian kernel in time\n",
    "    kconv_t = np.exp(-t**2*1.0/(sigt**2))\n",
    "    \n",
    "    # check for singularities in the definition\n",
    "    # if they exist, make the filter a delta function in time\n",
    "    if kconv_t.size > 1:\n",
    "        if np.sum(kconv_t) == 0:\n",
    "            it0 = np.argmin(np.abs(t))\n",
    "            kconv_t[it0] = 1.0\n",
    "    else: \n",
    "        kconv_t = np.ones(1)\n",
    "\n",
    "    # build Gaussian kernel in range\n",
    "    kconv_z = np.exp(-z**2*1.0/(sigz**2))\n",
    "    \n",
    "    # check for singularities in the definition\n",
    "    # if they exist, make the filter a delta function in range\n",
    "    if kconv_z.size > 1:\n",
    "        if np.sum(kconv_z) == 0:\n",
    "            iz0 = np.argmin(np.abs(z))\n",
    "            kconv_z[iz0] = 1.0\n",
    "    else:\n",
    "        kconv_z = np.ones(1)\n",
    "\n",
    "    # combine the time and range kernels\n",
    "    kconv = kconv_t[:,np.newaxis]*kconv_z[np.newaxis,:]\n",
    "\n",
    "    # normalize the area of the kernel to conserve energy\n",
    "    if norm:\n",
    "        kconv = kconv/(1.0*np.sum(kconv))\n",
    "\n",
    "    return kconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the photon counts from the netcdf file\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "profs = {}\n",
    "with xr.open_dataset(data_path+ncfile) as ds:\n",
    "    for lvar in lidar_profile_data:\n",
    "        profs[lvar] = ds[lvar].values\n",
    "    lidar_range = ds['range'].values.copy()\n",
    "    time = ds['time'].values.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook will run a demonstration case on the combined\n",
    "channel of the MPD HSRL operating at 770.  This is effectively\n",
    "equivilant to a backscatter lidar observation.\n",
    "\"\"\"\n",
    "demo_var = 'Combined_Counts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variety of features in the scene from this data file.  Thick low clouds are between time indices 1000 and 2000.  High ice clouds are seen intermittantly starting around time index 2500 until lower clouds arrive late in the day near time index 7500.  The sun rises around time index 4800 and several instances of high background occur due to the simultaneous presence of sun and clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a quicklook of the backscatter data\n",
    "fig,ax = plt.subplots(1,1,figsize=(18,4))\n",
    "ax.imshow(profs[demo_var][:,::-1].T,norm=LogNorm())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demonstration presented here focuses on optimization of range smoothing kernels, but kernels can similarly be optimized for time smoothing or 2D kernels consisting of range and time components.  \n",
    "\n",
    "Generally we perform both time and range optimization independently where each time bin has its own unique smoothing kernel and each range bin has its own uniquely determined time smoothing kernel.\n",
    "\n",
    "In order to compare methodology, thinning is performed once and different methods are subsequently applied to that same data.  Comparing results between two different thinned cases, even if they originate from the same profile is generally not effective because the offset in the negative log-likelihood is prone to variation between different thinning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itime = 150   # select a profile index for this demonstration case \n",
    "              # interesting cases: \n",
    "              #   thin high cloud: 400\n",
    "              #   thick cirrus: 150\n",
    "\n",
    "\n",
    "max_alt = 12e3\n",
    "\n",
    "imax_alt = np.argmin(np.abs(lidar_range-max_alt))\n",
    "            \n",
    "# assign the demonstration case to its own independent variable\n",
    "pdemo = (profs[demo_var][itime,3:imax_alt])[np.newaxis,:]  # remove bottom three bins from laser \"bang\"\n",
    "plidar_range = lidar_range[3:imax_alt]  # create a new range array for the bang-removed profile\n",
    "dz = np.mean(np.diff(plidar_range))  # store the range resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson Thinning\n",
    "Poisson thinning generates two statistically independent observations from one observation.  The resulting thinned profiles have the same underlying mean photon arrival rate driving the observations.  This is demonstrated below on the two thinned profiles are\n",
    "```\n",
    "pfit\n",
    "```\n",
    "used for applying the filter ($\\mathbf{f}$ in the publication) and \n",
    "```\n",
    "pver\n",
    "```\n",
    "used for scoring the the filtered result ($\\mathbf{g}$ in the publication).\n",
    "\n",
    "If the two observations have equal signals but uncorrelated noise, then\n",
    "$$E\\left[\\frac{\\mathbf{f}-\\mathbf{g}}{\\sqrt{\\sigma_f^2+\\sigma_g^2}}\\right] \\approx 0 $$\n",
    "and \n",
    "$$std\\left[\\frac{\\mathbf{f}-\\mathbf{g}}{\\sqrt{\\sigma_f^2+\\sigma_g^2}}\\right] \\approx 1 $$\n",
    "\n",
    "where for a Poisson observation $f$, the variance in the estimate of mean photons is estimated by $\\sigma_f^2 = f+1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poisson thin the raw photon counts of the profile\n",
    "pfit,pver = poisson_thin(pdemo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the thinned data\n",
    "# the two verify and fit data should have \n",
    "# statistically independent noise but common \n",
    "# underlying signals\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "ax.plot(pdemo.flatten(),plidar_range*1e-3,'k.',label='observation')\n",
    "ax.plot(pfit.flatten(),plidar_range*1e-3,'.',alpha=0.5,label='fit data')\n",
    "ax.plot(pver.flatten(),plidar_range*1e-3,'.',alpha=0.5,label='verify data')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('Range [km]')\n",
    "ax.set_xlabel('Photon Counts')\n",
    "ax.grid(b=True)\n",
    "ax.set_ylim([0,12])\n",
    "ax.legend(loc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original photon count data, thinned fit data, and thinned verification data are shown above.  The shapes of the thinned profiles are the same but the statistical noise is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the difference in photon counts between the\n",
    "# fit and verify profiles \n",
    "# compare this to the expected standard deviation\n",
    "# for uncorrelated Poisson observations\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "ax.plot(pfit.flatten()-pver.flatten(),plidar_range,'g.',label='Actual')\n",
    "ax.plot(np.sqrt(pfit+1+pver+1).flatten(),plidar_range,'k--',label='Expected std.')\n",
    "ax.plot(-np.sqrt(pfit+1+pver+1).flatten(),plidar_range,'k--')\n",
    "ax.set_ylabel('Range [m]')\n",
    "ax.set_xlabel('Photon Count Difference')\n",
    "ax.grid(b=True)\n",
    "ax.set_ylim([0,12e3])\n",
    "ax.legend(loc=2)\n",
    "\n",
    "# adjusting by the expected uncertainty results in a\n",
    "# a mean value near zero with\n",
    "# a standard deviation near one\n",
    "# further suggesting the signals are common but the\n",
    "# noise is statistically independent\n",
    "print('mean: %f'%np.mean((pfit-pver)/np.sqrt(pfit+1+pver+1)))\n",
    "print('std: %f'%np.std((pfit-pver)/np.sqrt(pfit+1+pver+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the difference between the two thinned profiles (green dots) with the theoretically estimated variance in the difference assuming both profiles are observations of Poisson random variables and they are uncorrelated. ($\\sqrt{\\sigma_f^2+\\sigma_g^2}$).  The theoretically estimated standard deviation appears representative of the scatter in the difference signal and the scattered signal appears to be zero mean.\n",
    "\n",
    "The standard deviation normalized mean and standard deviation are reported above these figures.  The mean is near zero and the standard deviation is near one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Optimal Filter\n",
    "The optimal filter kernel in a set is found by applying all the kernels to $\\mathbf{f}$ then scoring the result $\\tilde{\\mathbf{f}}$ against the verification profile $\\mathbf{g}$.  The score is calculated using the negative log-likelihood of a Poisson random variable\n",
    "$$\\mathcal{E} = \\tilde{\\mathbf{f}} - \\mathbf{g}\\ln \\tilde{\\mathbf{f}} $$\n",
    "The filter that produces the lowest negative log-likelihood is taken to be the best filter from the set for this particular scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Filter the raw profile\n",
    "\"\"\"\n",
    "\n",
    "# define range of filters to try\n",
    "filt_size = np.logspace(-1,1,40)\n",
    "\n",
    "# initialize the output arrays\n",
    "inv_ll = np.zeros(filt_size.size)\n",
    "inv_ll_fit = np.zeros(filt_size.size)\n",
    "filt_profs = np.zeros((filt_size.size,pfit.size))\n",
    "\n",
    "for ifilt,filter_width in enumerate(filt_size):\n",
    "    # get the convolution kernel\n",
    "    kern = build_Gaussian_kernel(0,filter_width)\n",
    "    \n",
    "    # normalize by the amount of points in a region\n",
    "    # to avoid edge effects\n",
    "    norm = np.ones(pfit.shape)\n",
    "    norm = scipy.signal.convolve2d(norm,kern,mode='same')\n",
    "    # apply the filter and normalize the result\n",
    "    # with the amount of data points contributing\n",
    "    pfilt = scipy.signal.convolve2d(pfit,kern,mode='same')/norm\n",
    "    \n",
    "    pfilt[pfilt==0] = 0.001 # avoid zero values that blow up the log function\n",
    "    \n",
    "    # save the results\n",
    "    filt_profs[ifilt,:] = pfilt\n",
    "    inv_ll[ifilt] = np.nansum(pfilt-pver*np.log(pfilt))\n",
    "    inv_ll_fit[ifilt] = np.nansum(pfilt-pfit*np.log(pfilt)) # for comparison, also store iLL using fit data as a basis\n",
    "\n",
    "# get the index to the solution\n",
    "isol = np.argmin(inv_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the resulting optimized profile over the raw data\n",
    "# also include an over-filtered example\n",
    "# in a separate plot show the negative log-likelihood as a function of filter width\n",
    "\n",
    "fig,ax = plt.subplots(2,1,figsize=(5,10))\n",
    "ax[0].plot(pver.flatten(),plidar_range*1e-3,'.',markersize=2,label='raw data')\n",
    "ax[0].plot(filt_profs[isol,:],plidar_range*1e-3,'k--',label='filtered')\n",
    "ax[0].plot(filt_profs[-1,:],plidar_range*1e-3,':',label='over filtered')\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_ylabel('Range [km]')\n",
    "ax[0].set_xlabel('Photon Counts')\n",
    "ax[0].grid(b=True)\n",
    "ax[0].set_ylim([0,12])\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(filt_size*dz,inv_ll*1e-3,'b.')\n",
    "ax[1].plot(filt_size[isol]*dz,inv_ll[isol]*1e-3,'gs')\n",
    "ax[1].plot(filt_size*dz,(inv_ll_fit-np.mean(inv_ll_fit)+np.mean(inv_ll))*1e-3,'rd')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].grid(b=True)\n",
    "ax[1].set_xlabel('Filter Width [m]')\n",
    "ax[1].set_ylabel('Negative Log-Likelihood x $10^{-3}$');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plots for publication\n",
    "\"\"\"\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(3.2,3.2))\n",
    "ax.plot(pdemo.flatten(),plidar_range*1e-3,'k.',markersize=2,label='observation')\n",
    "ax.plot(pfit.flatten(),plidar_range*1e-3,'.',markersize=2,alpha=0.5,label='fit data')\n",
    "ax.plot(pver.flatten(),plidar_range*1e-3,'.',markersize=2,alpha=0.5,label='verify data')\n",
    "ax.plot(filt_profs[isol,:],plidar_range*1e-3,'-',linewidth=1,label='filtered')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('Range [km]')\n",
    "ax.set_xlabel('Photon Counts')\n",
    "ax.grid(b=True)\n",
    "ax.set_ylim([0,12])\n",
    "ax.legend(loc=3,prop={'size': 8})\n",
    "# plt.savefig('../../plots/Thinned_and_Filt_Data.png',dpi=300,bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(3.1,2.1))\n",
    "ax.plot(filt_size*dz,inv_ll*1e-3,'b.',markersize=3,label='verification')\n",
    "ylimits = ax.get_ylim()  # let the actual ILL data set the axes limits\n",
    "ax.plot(filt_size[isol]*dz,inv_ll[isol]*1e-3,'gd',alpha=0.5)\n",
    "ax.plot(filt_size*dz,(inv_ll_fit-np.mean(inv_ll_fit)+np.mean(inv_ll))*1e-3,'rs',markersize=2,label='fit')\n",
    "ax.set_ylim(ylimits)\n",
    "ax.set_xscale('log')\n",
    "ax.grid(b=True)\n",
    "ax.set_xlabel('Filter Width [m]',fontsize=8)\n",
    "ax.set_ylabel('Negative Log-Likelihood',fontsize=8)\n",
    "ax.legend()\n",
    "# plt.savefig('../../plots/InvLL_Filter.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_size[isol]*dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top figure shows the verification data in blue dots and the optimally filtered data is in the blacked dashed line.  For comparison, an over filtered case is also shown as the orange dotted line.  Overfiltering is more effective as supressing the random errors, but also skews the signal, particularly where clouds are present.\n",
    "\n",
    "The bottom figure shows the negative log-likelihood for each filter kernel width (defined by the standard deviation of a Gaussian) where the minimum (optimal) value is indicated by the green diamond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data\n",
    "\n",
    "To see the value of Poisson thinning and verification with the Negative Log-likelihood, we use synthetic data.  Here we know the true signal and can compare the results obtained using Poisson thinning.\n",
    "\n",
    "We generate a relatively simple 1D profile where aerosols exist below 3 km and there is some enhancement in aerosol near the top of the boundary layer due to entrainment (similar to the true profile we saw earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz=37.5\n",
    "lidar_range_s = np.arange(dz*2,12037.5,dz)[np.newaxis,:]\n",
    "bg_s = 1e-8\n",
    "\n",
    "mult = 6e8\n",
    "\n",
    "smol = np.exp(-lidar_range_s/8e3)\n",
    "saer = np.zeros(lidar_range_s.shape)\n",
    "saer[lidar_range_s < 3e3] = 4\n",
    "saer += 10*np.exp(-(lidar_range_s-3e3)**2/80**2)\n",
    "\n",
    "# add cloud\n",
    "# Randomly generated cloud structure\n",
    "nstruc = (np.maximum(1,np.random.randn()*10+5)).astype(np.int) # generate number of structures\n",
    "calt = np.random.randn(nstruc)*1e3+7e3  # generate altitude of the structures centered on 7km\n",
    "csig = np.maximum(50,np.random.randn(nstruc)*0.2e3+0.2e3)  # generate structure widths\n",
    "cBS = np.maximum(0,np.random.randn(nstruc)*30+30)  # generate the structure backscatter\n",
    "\n",
    "# cloud data used for publication\n",
    "# generated by the above random structure generator\n",
    "# comment this block to use a randomly generated structure\n",
    "nstruc = 4\n",
    "calt = np.array([5192.16337811, 7690.21222657, 7882.29934153, 7531.27967589])\n",
    "csig = np.array([ 50., 232.4572076,  331.93039876, 277.80581039])\n",
    "cBS = np.array([25.90638071, 23.32572197, 48.87279623, 55.86160267])\n",
    "\n",
    "scloud = np.zeros(lidar_range_s.shape)\n",
    "for ci in range(nstruc):\n",
    "    scloud+=cBS[ci]*np.exp(-(lidar_range_s-calt[ci])**2/csig[ci]**2)\n",
    "\n",
    "signal = mult*((smol*(1+saer+scloud))/(lidar_range_s**2)+bg_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the cloud structure parameters used for the simulation\n",
    "print(nstruc)\n",
    "print(calt)\n",
    "print(csig)\n",
    "print(cBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include a realistic overlap function.  Otherwise the first bin dominates the error.\n",
    "overlap0 = np.array([0,8.26e-6,2.5e-4,1.22e-3,3.46e-3,7.73e-3,3.17e-2,\n",
    "                     9.09e-2,1.85e-1,3.04e-1,6.02e-1,9.52e-1,9.72e-1,\n",
    "                     9.85e-1,9.92e-1,1.0,1.0])\n",
    "overlap_range = np.array([50,100,200,300,400,500,800,1000,1300,1500,2000,3000,4000,5000,6000,8000,12000])\n",
    "overlap = np.interp(lidar_range_s,overlap_range,overlap0)\n",
    "\n",
    "signal = signal*overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Poisson observations of the signal\n",
    "pdemo_s = np.random.poisson(lam=signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poisson thin the raw photon counts of the profile\n",
    "pfit_s,pver_s = poisson_thin(pdemo_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the thinned data.\n",
    "# the two verify and fit data should have \n",
    "# statistically independent noise but common \n",
    "# underlying signals\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "ax.plot(pdemo_s.flatten(),lidar_range_s.flatten()*1e-3,'k.',label='observation')\n",
    "ax.plot(pfit_s.flatten(),lidar_range_s.flatten()*1e-3,'.',alpha=0.5,label='fit data')\n",
    "ax.plot(pver_s.flatten(),lidar_range_s.flatten()*1e-3,'.',alpha=0.5,label='verify data')\n",
    "ax.plot(signal.flatten(),lidar_range_s.flatten()*1e-3,'-',alpha=0.5,label='signal')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('Range [km]')\n",
    "ax.set_xlabel('Photon Counts')\n",
    "ax.grid(b=True)\n",
    "ax.set_ylim([0,12])\n",
    "ax.legend(loc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Filter the raw profile\n",
    "\"\"\"\n",
    "\n",
    "# define range of filters to try\n",
    "filt_size_s = np.linspace(0.3,3,30)\n",
    "\n",
    "# initialize the output arrays\n",
    "inv_ll_s = np.zeros(filt_size_s.size)\n",
    "inv_ll_rev = np.zeros(filt_size_s.size)\n",
    "mae_s = np.zeros(filt_size_s.size)\n",
    "mpe_s = np.zeros(filt_size_s.size)\n",
    "inv_ll_fit_s = np.zeros(filt_size_s.size)\n",
    "filt_profs_s = np.zeros((filt_size_s.size,pfit_s.size))\n",
    "\n",
    "for ifilt,filter_width in enumerate(filt_size_s):\n",
    "    # get the convolution kernel\n",
    "    kern = build_Gaussian_kernel(0,filter_width)\n",
    "    \n",
    "    # normalize by the amount of points in a region\n",
    "    # to avoid edge effects\n",
    "    norm = np.ones(pfit_s.shape)\n",
    "    norm = scipy.signal.convolve2d(norm,kern,mode='same')\n",
    "    # apply the filter and normalize the result\n",
    "    # with the amount of data points contributing\n",
    "    pfilt = (scipy.signal.convolve2d(pfit_s,kern,mode='same')/norm)\n",
    "    \n",
    "    #pfilt[pfilt==0] = 0.001 # avoid zero values that blow up the log function\n",
    "    \n",
    "    # save the results\n",
    "    filt_profs_s[ifilt,:] = pfilt\n",
    "    inv_ll_s[ifilt] = np.nansum(pfilt-pver_s*np.log(pfilt))  # actual negative log likelihood\n",
    "    inv_ll_fit_s[ifilt] = np.nansum(pfilt-pfit_s*np.log(pfilt)) # for comparison, also store nLL using fit data as a basis\n",
    "    inv_ll_rev[ifilt] = np.nansum(0.5*signal-pfilt*np.log(signal)) # for comparison, also store nLL using fit data as a basis\n",
    "\n",
    "# get the index to the solution\n",
    "isol_s = np.argmin(inv_ll_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the negative log-likelihood of the true signal for comparison later\n",
    "inv_ll_signal = np.nansum(0.5*signal-pver_s*np.log(0.5*signal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the results just as before.  The green square shows the optimal filter width as determined by the Poisson thinning with verification technique described previously.  \n",
    "\n",
    "We also plot the negative log-likelihood computed using fit data (red) instead of the verification data.  Because the noise is correlated, the function is monotonically increasing with increased filter width.  It penalizes stochastic noise suppression.  This is why we need a separate verification profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the resulting optimized profile over the raw data\n",
    "# also include an over-filtered example\n",
    "# in a separate plot show the negative log-likelihood as a function of filter width\n",
    "\n",
    "fig,ax = plt.subplots(2,1,figsize=(5,10))\n",
    "ax[0].plot(pver_s.flatten(),lidar_range_s.flatten()*1e-3,'.',markersize=2,label='raw data')\n",
    "ax[0].plot(filt_profs_s[isol_s,:],lidar_range_s.flatten()*1e-3,'k--',label='filtered')\n",
    "ax[0].plot(0.5*signal.flatten(),lidar_range_s.flatten()*1e-3,':',label='actual')\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_ylabel('Range [km]')\n",
    "ax[0].set_xlabel('Photon Counts')\n",
    "ax[0].grid(b=True)\n",
    "ax[0].set_ylim([0,12])\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(filt_size_s*dz,inv_ll_s*1e-3,'b.',label='verification nLL')\n",
    "ax[1].plot(filt_size_s[isol_s]*dz,inv_ll_s[isol_s]*1e-3,'gs',label='minimum')\n",
    "ax[1].plot(filt_size_s*dz,(inv_ll_fit_s-np.mean(inv_ll_fit_s)+np.mean(inv_ll_s))*1e-3,'rd',alpha=0.5,label='fit nLL')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].grid(b=True)\n",
    "ax[1].set_xlabel('Filter Width [m]')\n",
    "ax[1].set_ylabel('Negative Log-Likelihood x $10^{-3}$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(filt_size_s*dz,inv_ll_s*1e-3,'b.')\n",
    "ax1.plot(filt_size_s*dz,inv_ll_signal*1e-3*np.ones(filt_size_s.shape),'k--')\n",
    "ax1.grid(b=True)\n",
    "ax1.set_xlabel('Filter Width [m]')\n",
    "ax1.set_ylabel('Negative Log-Likelihood x $10^{-3}$',color='blue');\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(filt_size_s*dz,inv_ll_s*1e-3,'b.')\n",
    "ax1.grid(b=True)\n",
    "ax1.set_xlabel('Filter Width [m]')\n",
    "ax1.set_ylabel('Negative Log-Likelihood x $10^{-3}$',color='blue');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 = plt.subplots(1,1,figsize=(3.1,2.1))\n",
    "ax1.plot(filt_size_s*dz,inv_ll_s-np.mean(inv_ll_s),'b.')\n",
    "# ax1.plot(filt_size_s*dz,inv_ll_fit_s-np.mean(inv_ll_fit_s),'rs',markersize=3,alpha=0.6)\n",
    "\n",
    "ax1.plot(filt_size_s*dz,inv_ll_signal*np.ones(filt_size_s.shape)-np.mean(inv_ll_s),'k--')\n",
    "ax1.grid(b=True)\n",
    "ax1.set_xlabel('Filter Width [m]',fontsize=8)\n",
    "ax1.set_ylabel('$\\Delta$ Negative Log-Likelihood',color='black',fontsize=8);\n",
    "\n",
    "# plt.savefig('../../plots/InvLL_MAE_Filter_SynetheticC0.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(3.2,3.2))\n",
    "ax.plot(pdemo_s.flatten(),lidar_range_s.flatten()*1e-3,'s',markersize=1,label='observation')\n",
    "ax.plot(pver_s.flatten(),lidar_range_s.flatten()*1e-3,'d',markersize=1,label='verify data')\n",
    "ax.plot(pfit_s.flatten(),lidar_range_s.flatten()*1e-3,'.',markersize=1,label='fit data')\n",
    "ax.plot(filt_profs_s[isol_s,:],lidar_range_s.flatten()*1e-3,'k-',alpha=0.5,label='filtered')\n",
    "ax.plot(0.5*signal.flatten(),lidar_range_s.flatten()*1e-3,'--',alpha=0.8,label='actual')\n",
    "\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('Range [km]')\n",
    "ax.set_xlabel('Photon Counts')\n",
    "ax.grid(b=True)\n",
    "ax.set_ylim([0,12])\n",
    "ax.legend(loc=0,prop={'size': 6})\n",
    "# plt.savefig('../../plots/Synthetic_Profile_ComparisonC.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(3.2,3.2))\n",
    "ax[0].plot(pdemo_s.flatten(),lidar_range_s.flatten()*1e-3,'s',markersize=1,label='obs')\n",
    "ax[0].plot(pver_s.flatten(),lidar_range_s.flatten()*1e-3,'d',markersize=1,label='verify')\n",
    "ax[0].plot(pfit_s.flatten(),lidar_range_s.flatten()*1e-3,'.',markersize=1,label='fit')\n",
    "ax[0].plot(filt_profs_s[isol_s,:],lidar_range_s.flatten()*1e-3,'k-',alpha=0.5,label='opt filter')\n",
    "ax[0].plot(filt_profs_s[isol_s+5,:],lidar_range_s.flatten()*1e-3,'-',alpha=0.5,label='over filter')\n",
    "ax[0].plot(0.5*signal.flatten(),lidar_range_s.flatten()*1e-3,'--',label='actual')\n",
    "\n",
    "\n",
    "ax[1].plot(np.abs(0.5*pdemo_s-0.5*signal).flatten(),lidar_range_s.flatten()*1e-3,'s',markersize=1)\n",
    "ax[1].plot(np.abs(pver_s-0.5*signal).flatten(),lidar_range_s.flatten()*1e-3,'d',markersize=1)\n",
    "ax[1].plot(np.abs(pfit_s-0.5*signal).flatten(),lidar_range_s.flatten()*1e-3,'.',markersize=1)\n",
    "ax[1].plot(np.abs(filt_profs_s[isol_s,:]-0.5*signal.flatten()),lidar_range_s.flatten()*1e-3,'k',alpha=0.5)\n",
    "ax[1].plot(np.abs(filt_profs_s[isol_s+5,:]-0.5*signal.flatten()),lidar_range_s.flatten()*1e-3,'-',alpha=0.5)\n",
    "ax[1].set_xlabel('Absolute Error',fontsize=7)\n",
    "\n",
    "ax[1].grid(b=True)\n",
    "ax[1].set_ylim([0,12])\n",
    "labels = [item.get_text() for item in ax[1].get_yticklabels()]\n",
    "empty_string_labels = ['']*len(labels)\n",
    "ax[1].set_yticklabels(empty_string_labels)\n",
    "ax[1].tick_params(axis='x', labelsize=7)\n",
    "\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_ylabel('Range [km]',fontsize=7)\n",
    "ax[0].set_xlabel('Photon Counts',fontsize=7)\n",
    "ax[0].grid(b=True)\n",
    "ax[0].set_ylim([0,12])\n",
    "ax[0].legend(loc=0,prop={'size': 5})\n",
    "ax[0].tick_params(axis='y', labelsize=7)\n",
    "ax[0].tick_params(axis='x', labelsize=7)\n",
    "# plt.savefig('../../plots/Synthetic_Profile_Comparison_2panelC.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_over = isol_s+5 # index to use as overfitting example\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(3.2,3.2))\n",
    "ax[0].plot(pver_s.flatten(),lidar_range_s.flatten()*1e-3,'d',markersize=1,label='verify')\n",
    "ax[0].plot(filt_profs_s[isol_s,:],lidar_range_s.flatten()*1e-3,'k-',alpha=0.7,label='opt filter')\n",
    "ax[0].plot(filt_profs_s[i_over,:],lidar_range_s.flatten()*1e-3,'--',alpha=0.7,label='over filter')\n",
    "ax[0].plot(0.5*signal.flatten(),lidar_range_s.flatten()*1e-3,':',label='actual')\n",
    "\n",
    "ax[1].plot(np.abs(pver_s-0.5*signal).flatten(),lidar_range_s.flatten()*1e-3,'d',markersize=1)\n",
    "ax[1].plot(np.abs(filt_profs_s[isol_s,:]-0.5*signal.flatten()),lidar_range_s.flatten()*1e-3,'k',alpha=0.7)\n",
    "ax[1].plot(np.abs(filt_profs_s[i_over,:]-0.5*signal.flatten()),lidar_range_s.flatten()*1e-3,'--',alpha=0.7)\n",
    "ax[1].set_xlabel('Absolute Error',fontsize=7)\n",
    "\n",
    "\n",
    "ax[1].grid(b=True)\n",
    "ax[1].set_ylim([0,12])\n",
    "labels = [item.get_text() for item in ax[1].get_yticklabels()]\n",
    "empty_string_labels = ['']*len(labels)\n",
    "ax[1].set_yticklabels(empty_string_labels)\n",
    "ax[1].tick_params(axis='x', labelsize=7)\n",
    "\n",
    "\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_ylabel('Range [km]',fontsize=7)\n",
    "ax[0].set_xlabel('Photon Counts',fontsize=7)\n",
    "ax[0].grid(b=True)\n",
    "ax[0].set_ylim([0,12])\n",
    "ax[0].legend(loc=0,prop={'size': 5})\n",
    "ax[0].tick_params(axis='y', labelsize=7)\n",
    "ax[0].tick_params(axis='x', labelsize=7)\n",
    "# plt.savefig('../../plots/Synthetic_Profile_Comparison_3panelC.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Optimization for Backscatter Ratio\n",
    "Principles described above can be similarly applied to multiple channels in two dimensions.  The example below applies filter optimization to estimating backscatter ratio from an HSRL in low signal-to-noise (SNR) conditions.  There is a significant amount of solar background noise in the molecular and cobmined channels of the lidar, it's operating at low pulse energy and the potassium filter used in the molecular channel is less efficient than its rubidium counterpart.  This is a difficult scene to process.\n",
    "\n",
    "We start by loading both the combined and molecular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the photon counts from the netcdf file\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "profs = {}\n",
    "with xr.open_dataset(data_path+ncfile) as ds:\n",
    "    for lvar in lidar_profile_data:\n",
    "        profs[lvar] = ds[lvar].values\n",
    "    lidar_range = ds['range'].values.copy()\n",
    "    time = ds['time'].values.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_alt = 10e3\n",
    "\n",
    "imax_alt = np.argmin(np.abs(lidar_range-max_alt))\n",
    "\n",
    "hsrl_profs = {}\n",
    "for var in lidar_profile_data:\n",
    "    # assign the demonstration case to its own independent variable\n",
    "    ptemp = profs[var]      # remove bottom three bins from laser \"bang\"\n",
    "    hsrl_profs[var] = {}\n",
    "    hsrl_profs[var]['raw'] = ptemp.copy()\n",
    "    hsrl_profs[var]['bg'] = 0.5*np.mean(hsrl_profs[var]['raw'][:,-100:],axis=1)[:,np.newaxis]\n",
    "    \n",
    "    hsrl_profs[var]['raw']=hsrl_profs[var]['raw'][:,3:imax_alt]\n",
    "    \n",
    "    # Poisson thin the raw photon counts of the profile\n",
    "    hsrl_profs[var]['fit'],hsrl_profs[var]['ver'] = poisson_thin(hsrl_profs[var]['raw'])\n",
    "    \n",
    "hsrl_lidar_range = lidar_range[3:imax_alt]  # create a new range array for the bang-removed profile\n",
    "dz = np.mean(np.diff(hsrl_lidar_range))  # store the range resolution  \n",
    "dt = np.mean(np.diff(time))/np.timedelta64(1, 's')  # store the time resolution  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we would evaluate all combinations of filters in range and time, but in the interest of speed, faster and more practical to separate the optimization.  So first we optimize the smoothing in time.  Then optimize in range.  Note that the optimization is faster if we evaluate fewer filters.\n",
    "\n",
    "The optimization is applied independently to both molecular and combined channels.\n",
    "\n",
    "In general it is a good idea to remove known structure from the signals before applying smoothing to avoid having it limit the amout of smoothing.  For smoothing in time, we remove the background before applying the kernel, then add it back before evaluating the negative log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define range of filters to try\n",
    "time_filt_size = np.logspace(-1,3,100)\n",
    "\n",
    "# initialize the output arrays\n",
    "hsrl_inv_ll = {}\n",
    "hsrl_filt_profs = {}\n",
    "for ch in hsrl_profs:\n",
    "    hsrl_inv_ll[ch] = {'time':np.zeros((time_filt_size.size,hsrl_profs[ch]['fit'].shape[1]))}\n",
    "    hsrl_filt_profs[ch] = np.zeros(hsrl_profs[ch]['fit'].shape+(time_filt_size.size,))\n",
    "\n",
    "for ifilt,filter_width in enumerate(time_filt_size):\n",
    "    # get the convolution kernel\n",
    "    kern = build_Gaussian_kernel(filter_width,0)\n",
    "    \n",
    "    # normalize by the amount of points in a region\n",
    "    # to avoid edge effects\n",
    "    norm = np.ones(hsrl_profs['Combined_Counts']['fit'].shape)\n",
    "    norm = scipy.signal.convolve2d(norm,kern,mode='same')\n",
    "    \n",
    "    for ch in hsrl_profs:\n",
    "        # apply the filter and normalize the result\n",
    "        # with the amount of data points contributing\n",
    "        # remove the background to avoid having it limit the smoothing kernel\n",
    "        pfilt = scipy.signal.convolve2d(hsrl_profs[ch]['fit']-hsrl_profs[ch]['bg'],kern,mode='same')/norm\n",
    "        pfilt+=hsrl_profs[ch]['bg']\n",
    "        pfilt[pfilt==0] = 0.001 # avoid zero values that blow up the log function\n",
    "\n",
    "        # save the results\n",
    "        hsrl_filt_profs[ch][:,:,ifilt] = pfilt.copy()\n",
    "        hsrl_inv_ll[ch]['time'][ifilt,:] = np.nansum(pfilt-hsrl_profs[ch]['ver']*np.log(pfilt),axis=0)\n",
    "\n",
    "isol = {}\n",
    "for ch in hsrl_profs:\n",
    "    # get the index to the solution\n",
    "    isol[ch] = {}\n",
    "    isol[ch]['time'] = np.argmin(hsrl_inv_ll[ch]['time'],axis=0)\n",
    "    hsrl_profs[ch]['filtered'] = hsrl_profs[ch]['fit'].copy()\n",
    "    hsrl_profs[ch]['filtered'][:,np.arange(hsrl_filt_profs[ch].shape[1])] = \\\n",
    "                hsrl_filt_profs[ch][:,np.arange(hsrl_filt_profs[ch].shape[1]),isol[ch]['time']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "for ch in hsrl_profs:\n",
    "    plt.plot(time_filt_size[isol[ch]['time']]*dt,hsrl_lidar_range,'.',alpha=0.8,label=ch.replace('_Counts',' '))\n",
    "plt.xlabel('Kernel Width [s]')\n",
    "plt.ylabel('Altitude [m]')\n",
    "plt.xscale('log')\n",
    "plt.grid(b=True)\n",
    "plt.legend(loc=1)\n",
    "# plt.savefig('../../plots/Optimized_2D_Filter_Width_Time.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal smoothing kernels in time for each range bin are shown above.  Regions with clouds generally require smaller kernels to preserve the signal structure, where relatively clear altitudes use large kernels to suppress shot noise. \n",
    "\n",
    "We now optimize the range smoothing on the optimized output from time smoothing.  Similar to what we did when removing background, it can help to remove known structure in range before applying the kernel, then reapplying before evaluating the negative log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define range of filters to try\n",
    "range_filt_size = np.logspace(-1,1.5,40)\n",
    "\n",
    "# initialize the output arrays\n",
    "hsrl_filt_profs = {}\n",
    "for ch in hsrl_profs:\n",
    "    hsrl_filt_profs[ch] = np.zeros(hsrl_profs[ch]['fit'].shape+(range_filt_size.size,))\n",
    "    hsrl_inv_ll[ch]['range'] = np.zeros((range_filt_size.size,hsrl_profs[ch]['fit'].shape[0]))\n",
    "\n",
    "for ifilt,filter_width in enumerate(range_filt_size):\n",
    "    # get the convolution kernel\n",
    "    kern = build_Gaussian_kernel(0,filter_width)\n",
    "    \n",
    "    # normalize by the amount of points in a region\n",
    "    # to avoid edge effects\n",
    "    norm = np.ones(hsrl_profs['Combined_Counts']['fit'].shape)\n",
    "    norm = scipy.signal.convolve2d(norm,kern,mode='same')\n",
    "    \n",
    "    for ch in hsrl_profs:\n",
    "        # apply the filter and normalize the result\n",
    "        # with the amount of data points contributing\n",
    "        # remove the background to avoid having it limit the smoothing kernel\n",
    "        pfilt = scipy.signal.convolve2d(hsrl_profs[ch]['filtered'],kern,mode='same')/norm\n",
    "        pfilt[pfilt==0] = 0.001 # avoid zero values that blow up the log function\n",
    "\n",
    "        # save the results\n",
    "        hsrl_filt_profs[ch][:,:,ifilt] = pfilt.copy()\n",
    "        hsrl_inv_ll[ch]['range'][ifilt,:] = np.nansum(pfilt-hsrl_profs[ch]['ver']*np.log(pfilt),axis=1)\n",
    "\n",
    "for ch in hsrl_profs:\n",
    "    # get the index to the solution\n",
    "    isol[ch]['range'] = np.argmin(hsrl_inv_ll[ch]['range'],axis=0)\n",
    "    hsrl_profs[ch]['filtered'] = hsrl_profs[ch]['fit'].copy()\n",
    "    hsrl_profs[ch]['filtered'][np.arange(hsrl_filt_profs[ch].shape[0]),:] = \\\n",
    "                hsrl_filt_profs[ch][np.arange(hsrl_filt_profs[ch].shape[0]),:,isol[ch]['range']]\n",
    "    hsrl_profs[ch]['overfilt'] = hsrl_filt_profs[ch][:,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "for ch in hsrl_profs:\n",
    "    plt.plot(time,range_filt_size[isol[ch]['range']]*dz,'.',alpha=0.6,label=ch.replace('_Counts',' '))\n",
    "plt.ylabel('Kernel Width [m]')\n",
    "plt.xlabel('Time [UTC]')\n",
    "plt.yscale('log')\n",
    "plt.grid(b=True)\n",
    "plt.legend()\n",
    "# plt.savefig('../../plots/Optimized_2D_Filter_Width_Range.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for ch in hsrl_profs:\n",
    "    plt.figure()\n",
    "    plt.imshow((hsrl_profs[ch]['fit']-hsrl_profs[ch]['bg'])[:,::-1].T,norm=LogNorm())\n",
    "    plt.clim([1,1e3])\n",
    "    plt.figure()\n",
    "    plt.imshow((hsrl_profs[ch]['filtered']-hsrl_profs[ch]['bg'])[:,::-1].T,norm=LogNorm())\n",
    "    plt.clim([1,1e3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above show the background subtracted photon counts of unsmoothed and optimally smoothed profiles for both the combined (top two) and molecular channels (bottom two)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate the backscatter ratio for the difference cases\n",
    "\"\"\"\n",
    "\n",
    "mol_gain = 2.0\n",
    "backscatter_ratio_ver = (hsrl_profs['Combined_Counts']['ver']-hsrl_profs['Combined_Counts']['bg'])/ \\\n",
    "                        (hsrl_profs['Molecular_Counts']['ver']-hsrl_profs['Molecular_Counts']['bg'])/mol_gain\n",
    "backscatter_ratio = (hsrl_profs['Combined_Counts']['fit']-hsrl_profs['Combined_Counts']['bg'])/ \\\n",
    "                        (hsrl_profs['Molecular_Counts']['fit']-hsrl_profs['Molecular_Counts']['bg'])/mol_gain\n",
    "backscatter_ratio_filtered = (hsrl_profs['Combined_Counts']['filtered']-hsrl_profs['Combined_Counts']['bg'])/ \\\n",
    "                        (hsrl_profs['Molecular_Counts']['filtered']-hsrl_profs['Molecular_Counts']['bg'])/mol_gain\n",
    "    \n",
    "backscatter_ratio_overfilt = (hsrl_profs['Combined_Counts']['overfilt']-hsrl_profs['Combined_Counts']['bg'])/ \\\n",
    "                        (hsrl_profs['Molecular_Counts']['overfilt']-hsrl_profs['Molecular_Counts']['bg'])/mol_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(backscatter_ratio[:,::-1].T,norm=LogNorm())\n",
    "plt.clim(1,1e2)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(backscatter_ratio_filtered[:,::-1].T,norm=LogNorm())\n",
    "plt.clim(1,1e2)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(backscatter_ratio_overfilt[:,::-1].T,norm=LogNorm())\n",
    "plt.clim(1,1e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above compare the cases where backscatter ratio is estimated with no smoothing (top), optimized smoothing in time and range (middle) and optimized temporal smoothing but over smoothing in range (bottom).  The top and bottom figures show the extremes of error contributions from stochastic noise (top) and smearing (bottom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "ax.plot(backscatter_ratio[itime,:],hsrl_lidar_range,'.-',alpha=0.5,label='unfiltered')\n",
    "ax.plot(backscatter_ratio_filtered[itime,:],hsrl_lidar_range,'.-',label='optimized')\n",
    "ax.plot(backscatter_ratio_overfilt[itime,:],hsrl_lidar_range,'.-',alpha=0.5,label='over filtered')\n",
    "ax.set_ylim([0,10e3])\n",
    "ax.set_xlim([1,100])\n",
    "ax.set_xscale('log')\n",
    "ax.grid(b=True)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(3.2,3.2))\n",
    "ax.plot(backscatter_ratio[itime,:],hsrl_lidar_range*1e-3,'.-',alpha=0.5,markersize=3,label='unfiltered')\n",
    "ax.plot(backscatter_ratio_filtered[itime,:],hsrl_lidar_range*1e-3,'.-',markersize=3,label='optimized')\n",
    "ax.plot(backscatter_ratio_overfilt[itime,:],hsrl_lidar_range*1e-3,'.-',markersize=3,alpha=0.5,label='over filtered')\n",
    "ax.set_ylim([0,10])\n",
    "ax.set_xlim([1,100])\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('Altitude [km]')\n",
    "ax.set_xlabel('Backscatter Ratio')\n",
    "ax.grid(b=True)\n",
    "ax.legend(prop={'size': 8})\n",
    "# plt.savefig('../../plots/Optimized_2D_BSR_Profile.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,1,figsize=(3.2,3.5))\n",
    "for ch in hsrl_profs:\n",
    "    ax[0].plot(time,range_filt_size[isol[ch]['range']]*dz,'.',markersize=2,alpha=0.6,label=ch.replace('_Counts',' '))\n",
    "    ax[1].plot(hsrl_lidar_range,time_filt_size[isol[ch]['time']]*dt,'.',markersize=2,alpha=0.8,label=ch.replace('_Counts',' '))\n",
    "ax[0].set_ylabel('Kernel Width [m]')\n",
    "ax[0].set_xlabel('Time [UTC]')\n",
    "ax[0].tick_params(axis='x', labelsize=7)\n",
    "ax[0].grid(b=True)\n",
    "#ax[0].legend(prop={'size': 8})\n",
    "ax[1].set_ylabel('Kernel Width [s]')\n",
    "ax[1].set_xlabel('Altitude [m]')\n",
    "ax[1].tick_params(axis='x', labelsize=7)\n",
    "ax[1].grid(b=True)\n",
    "ax[1].legend(loc=7,prop={'size': 8})\n",
    "# plt.savefig('../../plots/Optimized_2D_Filter_Widths.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax1 = plt.subplot2grid((3, 3), (0, 0), colspan=2,rowspan=2)\n",
    "ax1.pcolor(time,hsrl_lidar_range*1e-3,backscatter_ratio_filtered.T,norm=LogNorm(),vmin=1,vmax=1e2)\n",
    "ax1.set_ylabel('Altitude [km]')\n",
    "ax1.set_title('Backscatter Ratio')\n",
    "ax1.get_xaxis().set_visible(False)\n",
    "ax1.tick_params(axis='y', labelsize=7)\n",
    "\n",
    "ax2 = plt.subplot2grid((3, 3), (2, 0), colspan=2)\n",
    "for ch in hsrl_profs:\n",
    "    ax2.plot(time,range_filt_size[isol[ch]['range']]*dz,'.',markersize=2,alpha=0.5,label=ch.replace('_Counts',' '))\n",
    "ax2.set_xlabel('Time [UTC]')\n",
    "ax2.set_ylabel('Range Resolution [m]')\n",
    "ax2.grid(b=True,which='both',axis='y')\n",
    "ax2.grid(b=True,which='major',axis='x')\n",
    "ax2.tick_params(axis='x', labelsize=7)\n",
    "ax2.tick_params(axis='y', labelsize=7)\n",
    "ax2.format_xdata = mdates.DateFormatter('%H:%M')\n",
    "\n",
    "\n",
    "ax3 = plt.subplot2grid((3, 3), (0, 2), rowspan=2)\n",
    "for ch in hsrl_profs:\n",
    "    ax3.plot(time_filt_size[isol[ch]['time']]*dt,hsrl_lidar_range,'.',markersize=2,alpha=0.5,label=ch.replace('_Counts',' '))\n",
    "ax3.set_xlabel('Time Resolution [s]')\n",
    "labels = [item.get_text() for item in ax3.get_yticklabels()]\n",
    "empty_string_labels = ['']*len(labels)\n",
    "ax3.set_yticklabels(empty_string_labels)\n",
    "\n",
    "ax3.grid(b=True,which='both',axis='x')\n",
    "ax3.grid(b=True,which='major',axis='y')\n",
    "\n",
    "\n",
    "# plt.savefig('../../plots/Optimized_2D_Filter_w_BSR.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = 'Combined_Counts'\n",
    "plt.figure()\n",
    "ax1 = plt.subplot2grid((6, 16), (0, 0), colspan=8,rowspan=2)\n",
    "cimg = ax1.pcolor(time,hsrl_lidar_range*1e-3,(hsrl_profs[ch]['fit']-hsrl_profs[ch]['bg']).T,norm=LogNorm(),vmin=1,vmax=1e3)\n",
    "ax1.set_ylabel('Altitude [km]',fontsize=7)\n",
    "ax1.get_xaxis().set_visible(False)\n",
    "ax1.tick_params(axis='y', labelsize=7)\n",
    "\n",
    "\n",
    "ax2 = plt.subplot2grid((6, 16), (2, 0), colspan=8,rowspan=2)\n",
    "ax2.pcolor(time,hsrl_lidar_range*1e-3,(hsrl_profs[ch]['filtered']-hsrl_profs[ch]['bg']).T,norm=LogNorm(),vmin=1,vmax=1e3)\n",
    "ax2.set_ylabel('Altitude [km]',fontsize=7)\n",
    "ax2.get_xaxis().set_visible(False)\n",
    "ax2.tick_params(axis='y', labelsize=7)\n",
    "\n",
    "\n",
    "ax4 = plt.subplot2grid((6, 16), (4, 0), colspan=8,rowspan=1)\n",
    "ax4.plot(time,range_filt_size[isol[ch]['range']]*dz,'.',markersize=2,alpha=0.5,label=ch.replace('_Counts',' '))\n",
    "ax4.set_xlabel('Time [UTC]',fontsize=7)\n",
    "ax4.set_ylabel('Filter\\nWidth [m]',fontsize=7)\n",
    "ax4.grid(b=True,which='both',axis='y')\n",
    "ax4.grid(b=True,which='major',axis='x')\n",
    "ax4.tick_params(axis='x', labelsize=7)\n",
    "ax4.tick_params(axis='y', labelsize=7)\n",
    "ax4.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "ax4.set_xlim(ax1.get_xlim())\n",
    "ax4.set_yscale('log')\n",
    "\n",
    "\n",
    "ax3 = plt.subplot2grid((6, 16), (2, 8), rowspan=2, colspan=3)\n",
    "ax3.plot(time_filt_size[isol[ch]['time']]*dt,hsrl_lidar_range*1e-3,'.',markersize=2,alpha=0.5,label=ch.replace('_Counts',' '))\n",
    "ax3.set_xlabel('Filter\\nWidth [s]',fontsize=7)\n",
    "labels = [item.get_text() for item in ax3.get_yticklabels()]\n",
    "empty_string_labels = ['']*len(labels)\n",
    "ax3.set_yticklabels(empty_string_labels)\n",
    "ax3.tick_params(axis='x', labelsize=7)\n",
    "#ax3.get_yaxis().set_visible(False)\n",
    "ax3.set_ylim(ax2.get_ylim())\n",
    "ax3.set_xscale('log')\n",
    "ax3.xaxis.set_major_locator(ticker.LogLocator(base=10.0,numticks=15))\n",
    "\n",
    "\n",
    "ax3.grid(b=True,which='both',axis='x')\n",
    "ax3.grid(b=True,which='major',axis='y')\n",
    "\n",
    "cax = plt.subplot2grid((6, 8), (0, 4), rowspan=2, colspan=1)\n",
    "cb = plt.colorbar(cimg, cax = cax)\n",
    "cb.set_label('Photon Counts',fontsize=7)\n",
    "cb.ax.tick_params(labelsize=7) \n",
    "\n",
    "# plt.savefig('../../plots/Optimized_2D_Filter_'+ch+'2.png',dpi=300,bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "02b24369-ab1a-4f0b-9d45-538d8c36357f",
    "theme": {}
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
