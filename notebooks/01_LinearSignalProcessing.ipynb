{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of linear signal processing in photon counting lidar\n",
    "This notebook was developed as a suppliment to the publication\n",
    "\n",
    "Matthew Hayman, Robert A. Stillwell and Scott M. Spuler, \n",
    "\"Optimization of linear photon counting lidar signal processing through Poisson thinning,\"\n",
    "submitted to Optics Letters *In Review*, DOI:TBD\n",
    "\n",
    "The notebook demonstrates the basic concepts for optimizing linear smoothing\n",
    "kernenls to a particular lidar scene by splitting an observed photon count\n",
    "profile into a \"fit\" and \"verification\" profile.  Smoothing kernels are applied\n",
    "to the \"fit\" profile then evaluated against the verification data set to find \n",
    "the optimal filter kernel.\n",
    "\n",
    "The code and functions contained in this notebook are available for public use\n",
    "so long as the original publication is referenced, but in the interest of independent verification, we strongly recommend that you write your own functions.  While every effort is made to provide accurate calcualtions, this code is not guaranteed to be free from errors.\n",
    "\n",
    "The data used in this example is from an NCAR MicroPulse DIAL (MPD) [1,2] using the potassium HSRL channels [3,4].  This \n",
    "data is provided for example purposes only and should not be used in scientific\n",
    "study.  No data quality assurance can be provided for these datasets.\n",
    "\n",
    "1. Spuler et al., \"Field-deployable diode-laser-based differential absorption lidar (DIAL) for profiling water vapor,\" Atmos. Meas. Tech., 8, 1073â€“1087, DOI:10.5194/amt-8-1073-2015 (2015).\n",
    "\n",
    "2. NCAR/EOL Remote Sensing Facility, \"NCAR MPD data. Version 1.0,\"Retrieved 9 Jan 2020, DOI:10.26023/MX0D-Z722-M406.\n",
    "\n",
    "3. Stillewell et al.,\"Demonstration of a combined differential absorption and high spectral resolution lidar for profiling atmospheric temperature,\" Opt. Express, 28, 71-93, DOI: 10.1364/OE.379804 (2020).\n",
    "\n",
    "4. Hayman and Spuler, \"Demonstration of a diode-laser-based high spectral resolution lidar (HSRL) for quantitative profiling of clouds and aerosols,\" Opt. Express, 25(24), A1096-A1110  DOI:10.1364/OE.25.0A1096 (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "fc80df9c-6bde-4141-8896-29bcc42292b9"
    }
   },
   "outputs": [],
   "source": [
    "# import python libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr  # not part of the standard python environemnt. if using anaconda: conda install xarray\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "94399d8d-a6a8-458e-9ba7-0c1462efd429"
    }
   },
   "outputs": [],
   "source": [
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "40194af8-85a4-4216-8510-ea17fbaa57b1"
    }
   },
   "outputs": [],
   "source": [
    "ncfile = 'mpd05.20181022T12300019921_20181022T15163019921.nc'\n",
    "# ncfile = 'mpd05.20181022T18032019921_20181022T20495019531.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of lidar profile names to be loaded\n",
    "lidar_profile_data = ['Combined_Counts','Molecular_Counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "0e595ff4-fda4-472b-9f61-514ce4a0c825"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function definitions for optimizing the filter\n",
    "\"\"\"\n",
    "\n",
    "def poisson_thin(pc_data,n=2):\n",
    "    \"\"\"\n",
    "    Poisson thin photon counting data\n",
    "    \n",
    "    inputs:\n",
    "        pc_data - array of raw photon count data\n",
    "            This is assumed to have a Poisson PDF\n",
    "        n - number of thinned profiles desired\n",
    "    outputs:\n",
    "        pc_list - list of each independent profile\n",
    "\n",
    "    \"\"\"\n",
    "    # the first thinned profile is calculated using a binomial random\n",
    "    # number generator to simulate \n",
    "    # flipping a coin to decide if a photon count is included in this\n",
    "    # profile or not\n",
    "    \n",
    "    pc_remain = pc_data.copy()\n",
    "    pc_list = []\n",
    "    for ai in range(n-1):\n",
    "        pc_list += [np.random.binomial(pc_remain.astype(np.int),1.0/(n-ai),size=pc_remain.shape)]\n",
    "        pc_remain -= pc_list[-1]\n",
    "    # the last profile is whatever photon counts are left\n",
    "    pc_list+= [pc_remain]\n",
    "    \n",
    "    return pc_list\n",
    "\n",
    "def build_Gaussian_kernel(sigt,sigz,norm=True):\n",
    "    \"\"\"\n",
    "    Generates a Gaussian convolution kernel for\n",
    "    standard deviations sigt and sigz in units of grid points.\n",
    "    sigt and sigz are defined in units of grid steps\n",
    "    \"\"\"        \n",
    "\n",
    "    \n",
    "    nt = np.round(4*sigt) # estimate size of time grid needed\n",
    "    nz = np.round(4*sigz) # estimate size of range grid needed\n",
    "    t = np.arange(-nt,nt+1) # create time grid     \n",
    "    z = np.arange(-nz,nz+1) # create range grid\n",
    "    \n",
    "    \n",
    "    # build Gaussian kernel in time\n",
    "    kconv_t = np.exp(-t**2*1.0/(sigt**2))\n",
    "    \n",
    "    # check for singularities in the definition\n",
    "    # if they exist, make the filter a delta function in time\n",
    "    if kconv_t.size > 1:\n",
    "        if np.sum(kconv_t) == 0:\n",
    "            it0 = np.argmin(np.abs(t))\n",
    "            kconv_t[it0] = 1.0\n",
    "    else: \n",
    "        kconv_t = np.ones(1)\n",
    "\n",
    "    # build Gaussian kernel in range\n",
    "    kconv_z = np.exp(-z**2*1.0/(sigz**2))\n",
    "    \n",
    "    # check for singularities in the definition\n",
    "    # if they exist, make the filter a delta function in range\n",
    "    if kconv_z.size > 1:\n",
    "        if np.sum(kconv_z) == 0:\n",
    "            iz0 = np.argmin(np.abs(z))\n",
    "            kconv_z[iz0] = 1.0\n",
    "    else:\n",
    "        kconv_z = np.ones(1)\n",
    "\n",
    "    # combine the time and range kernels\n",
    "    kconv = kconv_t[:,np.newaxis]*kconv_z[np.newaxis,:]\n",
    "\n",
    "    # normalize the area of the kernel to conserve energy\n",
    "    if norm:\n",
    "        kconv = kconv/(1.0*np.sum(kconv))\n",
    "\n",
    "    return kconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the photon counts from the netcdf file\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "profs = {}\n",
    "with xr.open_dataset(data_path+ncfile) as ds:\n",
    "    for lvar in lidar_profile_data:\n",
    "        profs[lvar] = ds[lvar].values\n",
    "    lidar_range = ds['range'].values.copy()\n",
    "    time = ds['time'].values.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook will run a demonstration case on the combined\n",
    "channel of the MPD HSRL operating at 770.  This is effectively\n",
    "equivilant to a backscatter lidar observation.\n",
    "\"\"\"\n",
    "demo_var = 'Combined_Counts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variety of features in the scene from this data file.  Thick low clouds are between time indices 1000 and 2000.  High ice clouds are seen intermittantly starting around time index 2500 until lower clouds arrive late in the day near time index 7500.  The sun rises around time index 4800 and several instances of high background occur due to the simultaneous presence of sun and clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a quicklook of the backscatter data\n",
    "fig,ax = plt.subplots(1,1,figsize=(18,4))\n",
    "ax.imshow(profs[demo_var][:,::-1].T,norm=LogNorm())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demonstration presented here focuses on optimization of range smoothing kernels, but kernels can similarly be optimized for time smoothing or 2D kernels consisting of range and time components.  \n",
    "\n",
    "Generally we perform both time and range optimization independently where each time bin has its own unique smoothing kernel and each range bin has its own uniquely determined time smoothing kernel.\n",
    "\n",
    "In order to compare methodology, thinning is performed once and different methods are subsequently applied to that same data.  Comparing results between two different thinned cases, even if they originate from the same profile is generally not effective because the offset in the negative log-likelihood is prone to variation between different thinning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itime = 150   # select a profile index for this demonstration case \n",
    "              # interesting cases: \n",
    "              #   thin high cloud: 400\n",
    "              #   thick cirrus: 150\n",
    "\n",
    "\n",
    "max_alt = 12e3\n",
    "\n",
    "imax_alt = np.argmin(np.abs(lidar_range-max_alt))\n",
    "            \n",
    "# assign the demonstration case to its own independent variable\n",
    "pdemo = (profs[demo_var][itime,3:imax_alt])[np.newaxis,:]  # remove bottom three bins from laser \"bang\"\n",
    "plidar_range = lidar_range[3:imax_alt]  # create a new range array for the bang-removed profile\n",
    "dz = np.mean(np.diff(plidar_range))  # store the range resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson Thinning\n",
    "Poisson thinning generates two statistically independent observations from one observation.  The resulting thinned profiles have the same underlying mean photon arrival rate driving the observations.  This is demonstrated below on the two thinned profiles are\n",
    "```\n",
    "pfit\n",
    "```\n",
    "used for applying the filter ($\\mathbf{f}$ in the publication) and \n",
    "```\n",
    "pver\n",
    "```\n",
    "used for scoring the the filtered result ($\\mathbf{g}$ in the publication).\n",
    "\n",
    "If the two observations have equal signals but uncorrelated noise, then\n",
    "$$E\\left[\\frac{\\mathbf{f}-\\mathbf{g}}{\\sqrt{\\sigma_f^2+\\sigma_g^2}}\\right] \\approx 0 $$\n",
    "and \n",
    "$$std\\left[\\frac{\\mathbf{f}-\\mathbf{g}}{\\sqrt{\\sigma_f^2+\\sigma_g^2}}\\right] \\approx 1 $$\n",
    "\n",
    "where for a Poisson observation $f$, the variance in the estimate of mean photons is estimated by $\\sigma_f^2 = f+1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poisson thin the raw photon counts of the profile\n",
    "pfit,pver = poisson_thin(pdemo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the thinned data\n",
    "# the two verify and fit data should have \n",
    "# statistically independent noise but common \n",
    "# underlying signals\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "ax.plot(pdemo.flatten(),plidar_range*1e-3,'k.',label='observation')\n",
    "ax.plot(pfit.flatten(),plidar_range*1e-3,'.',alpha=0.5,label='fit data')\n",
    "ax.plot(pver.flatten(),plidar_range*1e-3,'.',alpha=0.5,label='verify data')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('Range [km]')\n",
    "ax.set_xlabel('Photon Counts')\n",
    "ax.grid(b=True)\n",
    "ax.set_ylim([0,12])\n",
    "ax.legend(loc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original photon count data, thinned fit data, and thinned verification data are shown above.  The shapes of the thinned profiles are the same but the statistical noise is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the difference in photon counts between the\n",
    "# fit and verify profiles \n",
    "# compare this to the expected standard deviation\n",
    "# for uncorrelated Poisson observations\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "ax.plot(pfit.flatten()-pver.flatten(),plidar_range,'g.',label='Actual')\n",
    "ax.plot(np.sqrt(pfit+1+pver+1).flatten(),plidar_range,'k--',label='Expected std.')\n",
    "ax.plot(-np.sqrt(pfit+1+pver+1).flatten(),plidar_range,'k--')\n",
    "ax.set_ylabel('Range [m]')\n",
    "ax.set_xlabel('Photon Count Difference')\n",
    "ax.grid(b=True)\n",
    "ax.set_ylim([0,12e3])\n",
    "ax.legend(loc=2)\n",
    "\n",
    "# adjusting by the expected uncertainty results in a\n",
    "# a mean value near zero with\n",
    "# a standard deviation near one\n",
    "# further suggesting the signals are common but the\n",
    "# noise is statistically independent\n",
    "print('mean: %f'%np.mean((pfit-pver)/np.sqrt(pfit+1+pver+1)))\n",
    "print('std: %f'%np.std((pfit-pver)/np.sqrt(pfit+1+pver+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the difference between the two thinned profiles (green dots) with the theoretically estimated variance in the difference assuming both profiles are observations of Poisson random variables and they are uncorrelated. ($\\sqrt{\\sigma_f^2+\\sigma_g^2}$).  The theoretically estimated standard deviation appears representative of the scatter in the difference signal and the scattered signal appears to be zero mean.\n",
    "\n",
    "The standard deviation normalized mean and standard deviation are reported above these figures.  The mean is near zero and the standard deviation is near one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Optimal Filter\n",
    "We assume that photon count observations are Poisson distributed, which is typically a good assumption in photon counting lidar when the photon arrival rate is much less than the dead time or pulse width of the detection system (depending on the type of detector employed).  This means the probability distribution function relating the mean photon arrival rate $\\alpha$ to the observed counts $s$ is\n",
    "\n",
    "$$ P_{\\alpha}(s) = \\frac{e^{-\\alpha} \\alpha^s}{s!} $$.\n",
    "\n",
    "Here, the objective is to find $\\alpha$, given the observation $s$.  The value of $\\alpha$ that maximizes $P_{\\alpha}(s)$ for an observed $s$ is the maximum likelihood estimate.  This can be extended to the multivariate case, where we assume the structure in $\\alpha$ is correlated (like in an atmospheric scene) but the photon counts are not.  In that case, the total likelihood is the product of each individual point.\n",
    "\n",
    "$$ P_{\\tilde{\\alpha}}(\\mathbf{s}) = \\prod_i \\frac{e^{-\\tilde{\\alpha}_i} \\tilde{\\alpha}_i^{s_i}}{s_i!} $$\n",
    "\n",
    "The likelihood equation above should be maximized to find the most likely estimate of the mean photon count rate.  However we can equivalently minimize $-\\ln(P_{\\tilde{\\alpha}}(\\mathbf{s}))$ which is easier to represent with floating point numbers and puts it in the often preferred format for minimizing a cost function.  This gives the negative log-likelihood of a Poisson number below.\n",
    "\n",
    "$$ \\mathcal{E}(\\tilde{\\alpha}) = \\sum_i \\left[ \\tilde{\\alpha}_i-s_i \\ln \\tilde{\\alpha}_i\\right] $$\n",
    "\n",
    "In the application here, we estimate the mean photon arrival rate using linear smoothing applied to $\\mathbf{f}$ to obtain $\\tilde{\\mathbf{f}}$.  This estimate of the signal is evaluated against the other thinned copy of the data $\\mathbf{g}$ (which is very importantly statistically independent from $\\mathbf{f}$).  That gives the following equation for scoring our estimate of the mean photon photon counts.\n",
    "\n",
    "$$\\mathcal{E} = \\tilde{\\mathbf{f}} - \\mathbf{g}\\ln \\tilde{\\mathbf{f}} $$\n",
    "\n",
    "The optimal filter kernel is found by applying all the kernels in the set to $\\mathbf{f}$ then scoring the result $\\tilde{\\mathbf{f}}$ against the verification profile $\\mathbf{g}$.  The score is calculated using the negative log-likelihood of a Poisson random variable.  The filter that produces the lowest negative log-likelihood is taken to be the best filter from the set for this particular scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we evaluate several different Gaussian filters on a 1D profile using the method described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Filter the raw profile\n",
    "\"\"\"\n",
    "\n",
    "# define range of filters to try\n",
    "filt_size = np.logspace(-1,1,40)\n",
    "\n",
    "# initialize the output arrays\n",
    "neg_ll = np.zeros(filt_size.size)\n",
    "neg_ll_fit = np.zeros(filt_size.size)\n",
    "filt_profs = np.zeros((filt_size.size,pfit.size))\n",
    "rel_error_f = np.zeros(filt_size.size) # for POLS analysis\n",
    "rel_error_v = np.zeros(filt_size.size) # for POLS analysis\n",
    "\n",
    "for ifilt,filter_width in enumerate(filt_size):\n",
    "    # get the convolution kernel\n",
    "    kern = build_Gaussian_kernel(0,filter_width)\n",
    "    \n",
    "    # normalize by the amount of points in a region\n",
    "    # to avoid edge effects\n",
    "    norm = np.ones(pfit.shape)\n",
    "    norm = scipy.signal.convolve2d(norm,kern,mode='same')\n",
    "    # apply the filter and normalize the result\n",
    "    # with the amount of data points contributing\n",
    "    pfilt = scipy.signal.convolve2d(pfit,kern,mode='same')/norm\n",
    "    \n",
    "    pfilt[pfilt==0] = 0.001 # avoid zero values that create -infs in the log function\n",
    "    \n",
    "    # save the results\n",
    "    filt_profs[ifilt,:] = pfilt\n",
    "    neg_ll[ifilt] = np.nansum(pfilt-pver*np.log(pfilt))  # store the negative log-likelihood\n",
    "    neg_ll_fit[ifilt] = np.nansum(pfilt-pfit*np.log(pfilt)) # for comparison, also store negLL using fit data as a basis\n",
    "\n",
    "# get the index to the solution\n",
    "isol = np.argmin(neg_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the resulting optimized profile over the raw data\n",
    "# also include an over-filtered example\n",
    "# in a separate plot show the negative log-likelihood as a function of filter width\n",
    "\n",
    "fig,ax = plt.subplots(2,1,figsize=(5,10))\n",
    "ax[0].plot(pver.flatten(),plidar_range*1e-3,'.',markersize=2,label='raw data')\n",
    "ax[0].plot(filt_profs[isol,:],plidar_range*1e-3,'k--',label='filtered')\n",
    "ax[0].plot(filt_profs[-1,:],plidar_range*1e-3,':',label='over filtered')\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_ylabel('Range [km]')\n",
    "ax[0].set_xlabel('Photon Counts')\n",
    "ax[0].grid(b=True)\n",
    "ax[0].set_ylim([0,12])\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(filt_size*dz,neg_ll*1e-3,'b.')\n",
    "ax[1].plot(filt_size[isol]*dz,neg_ll[isol]*1e-3,'gs')\n",
    "ax[1].plot(filt_size*dz,(neg_ll_fit-np.mean(neg_ll_fit)+np.mean(neg_ll))*1e-3,'rd')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].grid(b=True)\n",
    "ax[1].set_xlabel('Filter Width [m]')\n",
    "ax[1].set_ylabel('Negative Log-Likelihood x $10^{-3}$');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plots for publication\n",
    "\"\"\"\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(3.2,3.2))\n",
    "ax.plot(pdemo.flatten(),plidar_range*1e-3,'k.',markersize=2,label='observation')\n",
    "ax.plot(pfit.flatten(),plidar_range*1e-3,'.',markersize=2,alpha=0.5,label='fit data')\n",
    "ax.plot(pver.flatten(),plidar_range*1e-3,'.',markersize=2,alpha=0.5,label='verify data')\n",
    "ax.plot(filt_profs[isol,:],plidar_range*1e-3,'-',linewidth=1,label='filtered')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('Range [km]')\n",
    "ax.set_xlabel('Photon Counts')\n",
    "ax.grid(b=True)\n",
    "ax.set_ylim([0,12])\n",
    "ax.legend(loc=3,prop={'size': 8})\n",
    "# plt.savefig('../../plots/Thinned_and_Filt_Data.png',dpi=300,bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(3.1,2.1))\n",
    "ax.plot(filt_size*dz,neg_ll*1e-3,'b.',markersize=3,label='verification')\n",
    "ylimits = ax.get_ylim()  # let the actual ILL data set the axes limits\n",
    "ax.plot(filt_size[isol]*dz,neg_ll[isol]*1e-3,'gd',alpha=0.5)\n",
    "ax.plot(filt_size*dz,(neg_ll_fit-np.mean(neg_ll_fit)+np.mean(neg_ll))*1e-3,'rs',markersize=2,label='fit')\n",
    "ax.set_ylim(ylimits)\n",
    "ax.set_xscale('log')\n",
    "ax.grid(b=True)\n",
    "ax.set_xlabel('Filter Width [m]',fontsize=8)\n",
    "ax.set_ylabel('Negative Log-Likelihood',fontsize=8)\n",
    "ax.legend()\n",
    "# plt.savefig('../../plots/negLL_Filter.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_size[isol]*dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top figure shows the verification data in blue dots and the optimally filtered data is in the blacked dashed line.  For comparison, an over filtered case is also shown as the orange dotted line.  Overfiltering is more effective as supressing the random errors, but also skews the signal, particularly where clouds are present.\n",
    "\n",
    "The bottom figure shows the negative log-likelihood for each filter kernel width (defined by the standard deviation of a Gaussian) where the minimum (optimal) value is indicated by the green diamond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data\n",
    "\n",
    "To see the value of Poisson thinning and verification with the Negative Log-likelihood, we use synthetic data.  Here we know the true signal and can compare the results obtained using Poisson thinning.\n",
    "\n",
    "We generate a relatively simple 1D profile where aerosols exist below 3 km and there is some enhancement in aerosol near the top of the boundary layer due to entrainment (similar to the true profile we saw earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz=37.5\n",
    "lidar_range_s = np.arange(dz*2,12037.5,dz)[np.newaxis,:]\n",
    "bg_s = 1e-8\n",
    "\n",
    "mult = 6e8\n",
    "\n",
    "smol = np.exp(-lidar_range_s/8e3)\n",
    "saer = np.zeros(lidar_range_s.shape)\n",
    "saer[lidar_range_s < 3e3] = 4\n",
    "saer += 10*np.exp(-(lidar_range_s-3e3)**2/80**2)\n",
    "\n",
    "# add cloud\n",
    "# Randomly generated cloud structure\n",
    "nstruc = (np.maximum(1,np.random.randn()*10+5)).astype(np.int) # generate number of structures\n",
    "calt = np.random.randn(nstruc)*1e3+7e3  # generate altitude of the structures centered on 7km\n",
    "csig = np.maximum(50,np.random.randn(nstruc)*0.2e3+0.2e3)  # generate structure widths\n",
    "cBS = np.maximum(0,np.random.randn(nstruc)*30+30)  # generate the structure backscatter\n",
    "\n",
    "# cloud data used for publication\n",
    "# generated by the above random structure generator\n",
    "# comment this block to use a randomly generated structure\n",
    "nstruc = 4\n",
    "calt = np.array([5192.16337811, 7690.21222657, 7882.29934153, 7531.27967589])\n",
    "csig = np.array([ 50., 232.4572076,  331.93039876, 277.80581039])\n",
    "cBS = np.array([25.90638071, 23.32572197, 48.87279623, 55.86160267])\n",
    "\n",
    "scloud = np.zeros(lidar_range_s.shape)\n",
    "for ci in range(nstruc):\n",
    "    scloud+=cBS[ci]*np.exp(-(lidar_range_s-calt[ci])**2/csig[ci]**2)\n",
    "\n",
    "signal = mult*((smol*(1+saer+scloud))/(lidar_range_s**2)+bg_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the cloud structure parameters used for the simulation\n",
    "print(nstruc)\n",
    "print(calt)\n",
    "print(csig)\n",
    "print(cBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include a realistic overlap function.  Otherwise the first bin dominates the error.\n",
    "overlap0 = np.array([0,8.26e-6,2.5e-4,1.22e-3,3.46e-3,7.73e-3,3.17e-2,\n",
    "                     9.09e-2,1.85e-1,3.04e-1,6.02e-1,9.52e-1,9.72e-1,\n",
    "                     9.85e-1,9.92e-1,1.0,1.0])\n",
    "overlap_range = np.array([50,100,200,300,400,500,800,1000,1300,1500,2000,3000,4000,5000,6000,8000,12000])\n",
    "overlap = np.interp(lidar_range_s,overlap_range,overlap0)\n",
    "\n",
    "signal = signal*overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Poisson observations of the signal\n",
    "pdemo_s = np.random.poisson(lam=signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poisson thin the raw photon counts of the profile\n",
    "pfit_s,pver_s = poisson_thin(pdemo_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the thinned data.\n",
    "# the two verify and fit data should have \n",
    "# statistically independent noise but common \n",
    "# underlying signals\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "ax.plot(pdemo_s.flatten(),lidar_range_s.flatten()*1e-3,'k.',label='observation')\n",
    "ax.plot(pfit_s.flatten(),lidar_range_s.flatten()*1e-3,'.',alpha=0.5,label='fit data')\n",
    "ax.plot(pver_s.flatten(),lidar_range_s.flatten()*1e-3,'.',alpha=0.5,label='verify data')\n",
    "ax.plot(signal.flatten(),lidar_range_s.flatten()*1e-3,'-',alpha=0.5,label='signal')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('Range [km]')\n",
    "ax.set_xlabel('Photon Counts')\n",
    "ax.grid(b=True)\n",
    "ax.set_ylim([0,12])\n",
    "ax.legend(loc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Filter the raw profile\n",
    "\"\"\"\n",
    "\n",
    "# define range of filters to try\n",
    "filt_size_s = np.linspace(0.3,3,30)\n",
    "\n",
    "# initialize the output arrays\n",
    "neg_ll_s = np.zeros(filt_size_s.size)\n",
    "neg_ll_rev = np.zeros(filt_size_s.size)\n",
    "mae_s = np.zeros(filt_size_s.size)\n",
    "mpe_s = np.zeros(filt_size_s.size)\n",
    "neg_ll_fit_s = np.zeros(filt_size_s.size)\n",
    "filt_profs_s = np.zeros((filt_size_s.size,pfit_s.size))\n",
    "\n",
    "for ifilt,filter_width in enumerate(filt_size_s):\n",
    "    # get the convolution kernel\n",
    "    kern = build_Gaussian_kernel(0,filter_width)\n",
    "    \n",
    "    # normalize by the amount of points in a region\n",
    "    # to avoid edge effects\n",
    "    norm = np.ones(pfit_s.shape)\n",
    "    norm = scipy.signal.convolve2d(norm,kern,mode='same')\n",
    "    # apply the filter and normalize the result\n",
    "    # with the amount of data points contributing\n",
    "    pfilt = (scipy.signal.convolve2d(pfit_s,kern,mode='same')/norm)\n",
    "    \n",
    "    #pfilt[pfilt==0] = 0.001 # avoid zero values that blow up the log function\n",
    "    \n",
    "    # save the results\n",
    "    filt_profs_s[ifilt,:] = pfilt\n",
    "    neg_ll_s[ifilt] = np.nansum(pfilt-pver_s*np.log(pfilt))  # actual negative log likelihood\n",
    "    neg_ll_fit_s[ifilt] = np.nansum(pfilt-pfit_s*np.log(pfilt)) # for comparison, also store nLL using fit data as a basis\n",
    "    neg_ll_rev[ifilt] = np.nansum(0.5*signal-pfilt*np.log(signal)) # for comparison, also store nLL using fit data as a basis\n",
    "\n",
    "# get the index to the solution\n",
    "isol_s = np.argmin(neg_ll_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the negative log-likelihood of the true signal for comparison later\n",
    "neg_ll_signal = np.nansum(0.5*signal-pver_s*np.log(0.5*signal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the results just as before.  The green square shows the optimal filter width as determined by the Poisson thinning with verification technique described previously.  \n",
    "\n",
    "We also plot the negative log-likelihood computed using fit data (red) instead of the verification data.  Because the noise is correlated, the function is monotonically increasing with increased filter width.  It penalizes stochastic noise suppression.  This is why we need a separate verification profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the resulting optimized profile over the raw data\n",
    "# also include an over-filtered example\n",
    "# in a separate plot show the negative log-likelihood as a function of filter width\n",
    "\n",
    "fig,ax = plt.subplots(2,1,figsize=(5,10))\n",
    "ax[0].plot(pver_s.flatten(),lidar_range_s.flatten()*1e-3,'.',markersize=2,label='raw data')\n",
    "ax[0].plot(filt_profs_s[isol_s,:],lidar_range_s.flatten()*1e-3,'k--',label='filtered')\n",
    "ax[0].plot(0.5*signal.flatten(),lidar_range_s.flatten()*1e-3,':',label='actual')\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_ylabel('Range [km]')\n",
    "ax[0].set_xlabel('Photon Counts')\n",
    "ax[0].grid(b=True)\n",
    "ax[0].set_ylim([0,12])\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(filt_size_s*dz,neg_ll_s*1e-3,'b.',label='verification nLL')\n",
    "ax[1].plot(filt_size_s[isol_s]*dz,neg_ll_s[isol_s]*1e-3,'gs',label='minimum')\n",
    "ax[1].plot(filt_size_s*dz,(neg_ll_fit_s-np.mean(neg_ll_fit_s)+np.mean(neg_ll_s))*1e-3,'rd',alpha=0.5,label='fit nLL')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].grid(b=True)\n",
    "ax[1].set_xlabel('Filter Width [m]')\n",
    "ax[1].set_ylabel('Negative Log-Likelihood x $10^{-3}$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 = plt.subplots(1,1,figsize=(3.1,2.1))\n",
    "ax1.plot(filt_size_s*dz,neg_ll_s-np.mean(neg_ll_s),'b.')\n",
    "# ax1.plot(filt_size_s*dz,neg_ll_fit_s-np.mean(neg_ll_fit_s),'rs',markersize=3,alpha=0.6)\n",
    "\n",
    "ax1.plot(filt_size_s*dz,neg_ll_signal*np.ones(filt_size_s.shape)-np.mean(neg_ll_s),'k--')\n",
    "ax1.grid(b=True)\n",
    "ax1.set_xlabel('Filter Width [m]',fontsize=8)\n",
    "ax1.set_ylabel('$\\Delta$ Negative Log-Likelihood',color='black',fontsize=8);\n",
    "\n",
    "# plt.savefig('../../plots/negLL_MAE_Filter_SynetheticC0_2.png',dpi=300,bbox_inches='tight')\n",
    "# plt.savefig('../../plots/negLL_MAE_Filter_SynetheticC0_2.tif',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(3.2,3.2))\n",
    "ax.plot(pdemo_s.flatten(),lidar_range_s.flatten()*1e-3,'s',markersize=1,label='observation')\n",
    "ax.plot(pver_s.flatten(),lidar_range_s.flatten()*1e-3,'d',markersize=1,label='verify data')\n",
    "ax.plot(pfit_s.flatten(),lidar_range_s.flatten()*1e-3,'.',markersize=1,label='fit data')\n",
    "ax.plot(filt_profs_s[isol_s,:],lidar_range_s.flatten()*1e-3,'k-',alpha=0.5,label='filtered')\n",
    "ax.plot(0.5*signal.flatten(),lidar_range_s.flatten()*1e-3,'--',alpha=0.8,label='actual')\n",
    "\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('Range [km]')\n",
    "ax.set_xlabel('Photon Counts')\n",
    "ax.grid(b=True)\n",
    "ax.set_ylim([0,12])\n",
    "ax.legend(loc=0,prop={'size': 6})\n",
    "# plt.savefig('../../plots/Synthetic_Profile_ComparisonC.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_over = isol_s+10 # index to use as overfitting example\n",
    "\n",
    "print('Optimized filter width: %f m'%(filt_size_s[isol_s]*dz))\n",
    "print('Overfit filter width: %f m'%(filt_size_s[i_over]*dz))\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(3.2,3.2))\n",
    "ax[0].plot(0.5*signal.flatten(),lidar_range_s.flatten()*1e-3,'k:',label='actual')\n",
    "ax[0].plot(pfit_s.flatten(),lidar_range_s.flatten()*1e-3,'d',markersize=1,alpha=0.3,label='under filter')\n",
    "ax[0].plot(filt_profs_s[isol_s,:],lidar_range_s.flatten()*1e-3,'s',alpha=0.3,markersize=1,label='opt filter')\n",
    "ax[0].plot(filt_profs_s[i_over,:],lidar_range_s.flatten()*1e-3,'o',alpha=0.3,markersize=1,label='over filter')\n",
    "\n",
    "\n",
    "ax[1].plot(np.abs(pfit_s-0.5*signal).flatten(),lidar_range_s.flatten()*1e-3,'d',markersize=1,alpha=0.3,label='under filter')\n",
    "ax[1].plot(np.abs(filt_profs_s[isol_s,:]-0.5*signal.flatten()),lidar_range_s.flatten()*1e-3,'s',markersize=1,alpha=0.3,label='opt filter')\n",
    "ax[1].plot(np.abs(filt_profs_s[i_over,:]-0.5*signal.flatten()),lidar_range_s.flatten()*1e-3,'o',markersize=1,alpha=0.3,label='over filter')\n",
    "ax[1].set_xlabel('Absolute Error',fontsize=7)\n",
    "\n",
    "\n",
    "ax[1].grid(b=True)\n",
    "ax[1].set_ylim([0,12])\n",
    "labels = [item.get_text() for item in ax[1].get_yticklabels()]\n",
    "empty_string_labels = ['']*len(labels)\n",
    "ax[1].set_yticklabels(empty_string_labels)\n",
    "ax[1].tick_params(axis='x', labelsize=7)\n",
    "ax[1].set_xscale('log')\n",
    "\n",
    "\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_ylabel('Range [km]',fontsize=7)\n",
    "ax[0].set_xlabel('Photon Counts',fontsize=7)\n",
    "ax[0].grid(b=True)\n",
    "ax[0].set_ylim([0,12])\n",
    "ax[0].legend(loc=0,prop={'size': 5})\n",
    "ax[0].tick_params(axis='y', labelsize=7)\n",
    "ax[0].tick_params(axis='x', labelsize=7)\n",
    "# plt.savefig('../../plots/Synthetic_Profile_Comparison_3panelC_v2.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the publication we smooth the error plot to make sense of the statistical error.  This makes it much easier to see what the average error is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_over = isol_s+10 # index to use as overfitting example\n",
    "\n",
    "print('Optimized filter width: %f m'%(filt_size_s[isol_s]*dz))\n",
    "print('Overfit filter width: %f m'%(filt_size_s[i_over]*dz))\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(3.2,3.2))\n",
    "ax[0].plot(0.5*signal.flatten(),lidar_range_s.flatten()*1e-3,'k:',label='actual',linewidth=1)\n",
    "ax[0].plot(pfit_s.flatten(),lidar_range_s.flatten()*1e-3,'d',markersize=1,alpha=0.3,label='under')\n",
    "ax[0].plot(filt_profs_s[i_over,:],lidar_range_s.flatten()*1e-3,'o',alpha=0.3,markersize=1,label='over')\n",
    "ax[0].plot(filt_profs_s[isol_s,:],lidar_range_s.flatten()*1e-3,'s',alpha=0.3,markersize=1,label='optimal')\n",
    "\n",
    "sm_kern = np.ones(11)\n",
    "sm_kern = sm_kern/np.sum(sm_kern)\n",
    "scale_factor = np.convolve(np.ones(np.abs(pfit_s-0.5*signal).flatten().shape),sm_kern,mode='same')\n",
    "\n",
    "ax[1].plot(np.convolve(np.abs(pfit_s-0.5*signal).flatten(),sm_kern,mode='same')/scale_factor,lidar_range_s.flatten()*1e-3,'d',markersize=1,alpha=0.3,label='under')\n",
    "ax[1].plot(np.convolve(np.abs(filt_profs_s[i_over,:]-0.5*signal.flatten()),sm_kern,mode='same')/scale_factor,lidar_range_s.flatten()*1e-3,'o',markersize=1,alpha=0.3,label='over')\n",
    "ax[1].plot(np.convolve(np.abs(filt_profs_s[isol_s,:]-0.5*signal.flatten()),sm_kern,mode='same')/scale_factor,lidar_range_s.flatten()*1e-3,'s',markersize=1,alpha=0.3,label='optimal')\n",
    "ax[1].set_xlabel('Mean Absolute Error\\n[Photon Counts]',fontsize=7)\n",
    "\n",
    "\n",
    "ax[1].grid(b=True)\n",
    "ax[1].set_ylim([0,12])\n",
    "labels = [item.get_text() for item in ax[1].get_yticklabels()]\n",
    "empty_string_labels = ['']*len(labels)\n",
    "ax[1].set_yticklabels(empty_string_labels)\n",
    "ax[1].tick_params(axis='x', labelsize=7)\n",
    "ax[1].set_xscale('log')\n",
    "\n",
    "\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_ylabel('Range [km]',fontsize=7)\n",
    "ax[0].set_xlabel('Photon Counts',fontsize=7)\n",
    "ax[0].grid(b=True)\n",
    "ax[0].set_ylim([0,12])\n",
    "ax[0].legend(loc=0,prop={'size': 5})\n",
    "ax[0].tick_params(axis='y', labelsize=7)\n",
    "ax[0].tick_params(axis='x', labelsize=7)\n",
    "# plt.savefig('../../plots/Synthetic_Profile_Comparison_3panelCsmoothed_v2.png',dpi=300,bbox_inches='tight')\n",
    "# plt.savefig('../../plots/Synthetic_Profile_Comparison_3panelCsmoothed_v2.tif',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Optimization for Backscatter Ratio\n",
    "Principles described above can be similarly applied to multiple channels in two dimensions.  The example below applies filter optimization to estimating backscatter ratio from an HSRL in low signal-to-noise (SNR) conditions.  There is a significant amount of solar background noise in the molecular and cobmined channels of the lidar.  It operates at low pulse energy and the potassium filter used in the molecular channel is less efficient than its rubidium counterpart.  This is a difficult scene to process.\n",
    "\n",
    "We start by loading both the combined and molecular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the photon counts from the netcdf file\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "profs = {}\n",
    "with xr.open_dataset(data_path+ncfile) as ds:\n",
    "    for lvar in lidar_profile_data:\n",
    "        profs[lvar] = ds[lvar].values\n",
    "    lidar_range = ds['range'].values.copy()\n",
    "    time = ds['time'].values.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define time of filters to try (width defined in pixels)\n",
    "time_filt_size = np.logspace(-1,3,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define range of filters to try (width defined in pixels)\n",
    "range_filt_size = np.logspace(-1,1.5,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove filter definitions that are too similar to other filters.  This isn't absolutely necessary but it saves time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Range kernels')\n",
    "print('Original size: %d'%range_filt_size.size)\n",
    "klast = np.array([])\n",
    "del_kern = np.zeros(range_filt_size.size,dtype='bool')\n",
    "for isig,sigr in enumerate(range_filt_size):\n",
    "    kern = build_Gaussian_kernel(0,sigr)\n",
    "    if klast.size == kern.size:\n",
    "        if np.abs(kern-klast).max() < 1e-2:\n",
    "            del_kern[isig] = True\n",
    "    klast = kern.copy()\n",
    "range_filt_size = np.delete(range_filt_size,np.nonzero(del_kern)[0])\n",
    "\n",
    "print('New size: %d'%range_filt_size.size)\n",
    "\n",
    "print('Time kernels')\n",
    "print('Original size: %d'%time_filt_size.size)\n",
    "klast = np.array([])\n",
    "del_kern = np.zeros(time_filt_size.size,dtype='bool')\n",
    "for isig,sigt in enumerate(time_filt_size):\n",
    "    kern = build_Gaussian_kernel(0,sigt)\n",
    "    if klast.size == kern.size:\n",
    "        if np.abs(kern-klast).max() < 1e-2:\n",
    "            del_kern[isig] = True\n",
    "    klast = kern.copy()\n",
    "time_filt_size = np.delete(time_filt_size,np.nonzero(del_kern)[0])\n",
    "\n",
    "print('New size: %d'%time_filt_size.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the size of the profiles and apply Poisson thinning.\n",
    "\n",
    "Unlike the previous cases, we thin to 3 profiles instead of 2.  Since we don't know the \"true\" signal, we can use the third profile as a test dataset to evaluate different filter methods.  This test dataset is statistically independent from the fit and validation datasets used to set the tuning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_alt = 10e3\n",
    "\n",
    "imax_alt = np.argmin(np.abs(lidar_range-max_alt))\n",
    "\n",
    "hsrl_profs = {}\n",
    "for var in lidar_profile_data:\n",
    "    # assign the demonstration case to its own independent variable\n",
    "    ptemp = profs[var]      # remove bottom three bins from laser \"bang\"\n",
    "    hsrl_profs[var] = {}\n",
    "    hsrl_profs[var]['raw'] = ptemp.copy()\n",
    "    hsrl_profs[var]['bg'] = 1/3*np.mean(hsrl_profs[var]['raw'][:,-100:],axis=1)[:,np.newaxis]\n",
    "    \n",
    "    hsrl_profs[var]['raw']=hsrl_profs[var]['raw'][:,3:imax_alt]\n",
    "    \n",
    "    # Poisson thin the raw photon counts of the profile\n",
    "    hsrl_profs[var]['fit'],hsrl_profs[var]['ver'],hsrl_profs[var]['test'] = poisson_thin(hsrl_profs[var]['raw'],n=3)\n",
    "    \n",
    "hsrl_lidar_range = lidar_range[3:imax_alt]  # create a new range array for the bang-removed profile\n",
    "dz = np.mean(np.diff(hsrl_lidar_range))  # store the range resolution  \n",
    "dt = np.mean(np.diff(time))/np.timedelta64(1, 's')  # store the time resolution  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the smoothed scene using a static filter based on the resolution in Hayman and Spuler 2017 [4].  We will use this as a basis for comparison to the optimized method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_width_t = 1*60/dt\n",
    "filter_width_r = 37.5/dz\n",
    "# get the convolution kernel\n",
    "# kern = build_Gaussian_kernel(filter_width_t,filter_width_r)\n",
    "\n",
    "kern = np.ones((np.int(np.round(filter_width_t)),np.int(np.round(filter_width_r))))\n",
    "\n",
    "hsrl_fixedfilt_profs = {}\n",
    "hsrl_fixedfilt_profs_negll = {}\n",
    "\n",
    "# normalize by the amount of points in a region\n",
    "# to avoid edge effects\n",
    "norm = np.ones(hsrl_profs['Combined_Counts']['fit'].shape)\n",
    "norm = scipy.signal.convolve2d(norm,kern,mode='same')\n",
    "\n",
    "for ch in hsrl_profs:\n",
    "    # apply the filter and normalize the result\n",
    "    # with the amount of data points contributing\n",
    "    # remove the background to avoid having it limit the smoothing kernel\n",
    "    pfilt = scipy.signal.convolve2d(hsrl_profs[ch]['fit']-hsrl_profs[ch]['bg'],kern,mode='same')/norm\n",
    "    pfilt+=hsrl_profs[ch]['bg']\n",
    "    pfilt[pfilt==0] = 0.001 # avoid zero values that blow up the log function\n",
    "\n",
    "    # save the results\n",
    "    hsrl_fixedfilt_profs[ch] = pfilt.copy()\n",
    "    hsrl_fixedfilt_profs_negll[ch] = np.nansum(pfilt-hsrl_profs[ch]['ver']*np.log(pfilt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we would evaluate all combinations of filters in range and time, but its faster and more practical to separate the optimization.  So first we optimize the smoothing in time.  Then optimize in range.  Note that the optimization is faster if we evaluate fewer filters.\n",
    "\n",
    "The optimization is applied independently to both molecular and combined channels.\n",
    "\n",
    "In general it is a good idea to remove known structure from the signals before applying smoothing to avoid having those known structures limit the amout of smoothing.  For smoothing in time, we remove the background before applying the kernel, then add it back before evaluating the negative log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_filt_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define time of filters to try\n",
    "# time_filt_size = np.logspace(-1,3,100)\n",
    "\n",
    "# initialize the output arrays\n",
    "hsrl_neg_ll = {}\n",
    "hsrl_filt_profs = {}\n",
    "for ch in hsrl_profs:\n",
    "    hsrl_neg_ll[ch] = {'time':np.zeros((time_filt_size.size,hsrl_profs[ch]['fit'].shape[1]))}\n",
    "    hsrl_filt_profs[ch] = np.zeros(hsrl_profs[ch]['fit'].shape+(time_filt_size.size,))\n",
    "\n",
    "for ifilt,filter_width in enumerate(time_filt_size):\n",
    "    # get the convolution kernel\n",
    "    kern = build_Gaussian_kernel(filter_width,0)\n",
    "    \n",
    "    # normalize by the amount of points in a region\n",
    "    # to avoid edge effects\n",
    "    norm = np.ones(hsrl_profs['Combined_Counts']['fit'].shape)\n",
    "    norm = scipy.signal.convolve2d(norm,kern,mode='same')\n",
    "    \n",
    "    for ch in hsrl_profs:\n",
    "        # apply the filter and normalize the result\n",
    "        # with the amount of data points contributing\n",
    "        # remove the background to avoid having it limit the smoothing kernel\n",
    "        pfilt = scipy.signal.convolve2d(hsrl_profs[ch]['fit']-hsrl_profs[ch]['bg'],kern,mode='same')/norm\n",
    "        pfilt+=hsrl_profs[ch]['bg']\n",
    "        pfilt[pfilt==0] = 0.001 # avoid zero values that blow up the log function\n",
    "\n",
    "        # save the results\n",
    "        hsrl_filt_profs[ch][:,:,ifilt] = pfilt.copy()\n",
    "        hsrl_neg_ll[ch]['time'][ifilt,:] = np.nansum(pfilt-hsrl_profs[ch]['ver']*np.log(pfilt),axis=0)\n",
    "\n",
    "isol = {}\n",
    "for ch in hsrl_profs:\n",
    "    # get the index to the solution\n",
    "    isol[ch] = {}\n",
    "    isol[ch]['time'] = np.argmin(hsrl_neg_ll[ch]['time'],axis=0)\n",
    "    hsrl_profs[ch]['filtered'] = hsrl_profs[ch]['fit'].copy()\n",
    "    hsrl_profs[ch]['filtered'][:,np.arange(hsrl_filt_profs[ch].shape[1])] = \\\n",
    "                hsrl_filt_profs[ch][:,np.arange(hsrl_filt_profs[ch].shape[1]),isol[ch]['time']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_filt_list += [isol['Combined_Counts']['time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "for ch in hsrl_profs:\n",
    "    plt.plot(time_filt_size[isol[ch]['time']]*dt,hsrl_lidar_range,'.',alpha=0.8,label=ch.replace('_Counts',' '))\n",
    "plt.xlabel('Kernel Width [s]')\n",
    "plt.ylabel('Altitude [m]')\n",
    "plt.xscale('log')\n",
    "plt.grid(b=True)\n",
    "plt.legend(loc=1)\n",
    "# plt.savefig('../../plots/Optimized_2D_Filter_Width_Time.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal smoothing kernels in time for each range bin are shown above.  Regions with clouds generally require smaller kernels to preserve the signal structure, where relatively clear altitudes use large kernels to suppress shot noise. \n",
    "\n",
    "We now optimize the range smoothing on the optimized output from time smoothing.  Similar to what we did when removing background, it can help to remove known structure in range before applying the kernel, then reapplying before evaluating the negative log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define range of filters to try\n",
    "# range_filt_size = np.logspace(-1,1.5,40)\n",
    "\n",
    "# initialize the output arrays\n",
    "hsrl_filt_profs = {}\n",
    "for ch in hsrl_profs:\n",
    "    hsrl_filt_profs[ch] = np.zeros(hsrl_profs[ch]['fit'].shape+(range_filt_size.size,))\n",
    "    hsrl_neg_ll[ch]['range'] = np.zeros((range_filt_size.size,hsrl_profs[ch]['fit'].shape[0]))\n",
    "\n",
    "for ifilt,filter_width in enumerate(range_filt_size):\n",
    "    # get the convolution kernel\n",
    "    kern = build_Gaussian_kernel(0,filter_width)\n",
    "    \n",
    "    # normalize by the amount of points in a region\n",
    "    # to avoid edge effects\n",
    "    norm = np.ones(hsrl_profs['Combined_Counts']['fit'].shape)\n",
    "    norm = scipy.signal.convolve2d(norm,kern,mode='same')\n",
    "    \n",
    "    for ch in hsrl_profs:\n",
    "        # apply the filter and normalize the result\n",
    "        # with the amount of data points contributing\n",
    "        # remove the background to avoid having it limit the smoothing kernel\n",
    "        pfilt = scipy.signal.convolve2d(hsrl_profs[ch]['filtered'],kern,mode='same')/norm\n",
    "        pfilt[pfilt==0] = 0.001 # avoid zero values that blow up the log function\n",
    "\n",
    "        # save the results\n",
    "        hsrl_filt_profs[ch][:,:,ifilt] = pfilt.copy()\n",
    "        hsrl_neg_ll[ch]['range'][ifilt,:] = np.nansum(pfilt-hsrl_profs[ch]['ver']*np.log(pfilt),axis=1)\n",
    "\n",
    "for ch in hsrl_profs:\n",
    "    # get the index to the solution\n",
    "    isol[ch]['range'] = np.argmin(hsrl_neg_ll[ch]['range'],axis=0)\n",
    "    hsrl_profs[ch]['filtered'] = hsrl_profs[ch]['fit'].copy()\n",
    "    hsrl_profs[ch]['filtered'][np.arange(hsrl_filt_profs[ch].shape[0]),:] = \\\n",
    "                hsrl_filt_profs[ch][np.arange(hsrl_filt_profs[ch].shape[0]),:,isol[ch]['range']]\n",
    "    hsrl_profs[ch]['overfilt'] = hsrl_filt_profs[ch][:,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "for ch in hsrl_profs:\n",
    "    plt.plot(time,range_filt_size[isol[ch]['range']]*dz,'.',alpha=0.6,label=ch.replace('_Counts',' '))\n",
    "plt.ylabel('Kernel Width [m]')\n",
    "plt.xlabel('Time [UTC]')\n",
    "plt.yscale('log')\n",
    "plt.grid(b=True)\n",
    "plt.legend()\n",
    "# plt.savefig('../../plots/Optimized_2D_Filter_Width_Range.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows the optimal range filter size.  We can see that liquid clouds around 13:55 limit the kernel size quite a bit.\n",
    "\n",
    "As a sanity check we do a quick plot of the three main cases below.  Top to bottom the plots are Molecular unfiltered, Molecular optmizied, Molecular fixed filter, Combined unfiltered, Combined optimized and Combined fixed filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for ch in hsrl_profs:\n",
    "    plt.figure()\n",
    "    plt.imshow((hsrl_profs[ch]['fit']-hsrl_profs[ch]['bg'])[:,::-1].T,norm=LogNorm())\n",
    "    plt.clim([1,1e3])\n",
    "    plt.figure()\n",
    "    plt.imshow((hsrl_profs[ch]['filtered']-hsrl_profs[ch]['bg'])[:,::-1].T,norm=LogNorm())\n",
    "    plt.clim([1,1e3])\n",
    "    plt.figure()\n",
    "    plt.imshow((hsrl_fixedfilt_profs[ch]-hsrl_profs[ch]['bg'])[:,::-1].T,norm=LogNorm())\n",
    "    plt.clim([1,1e3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine which profile is the best representation (unfiltered, optimized or fixed filter), we compute the negative log-likelihood using the test data as the data to validate against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = 'Combined_Counts'\n",
    "negLL_fixed = hsrl_fixedfilt_profs_negll[ch]\n",
    "negLL_optimized = np.nansum(hsrl_profs[ch]['filtered']-hsrl_profs[ch]['test']*np.log(hsrl_profs[ch]['filtered']))\n",
    "negLL_unfiltered = np.nansum(np.maximum(hsrl_profs[ch]['fit'],0.001)-hsrl_profs[ch]['test']*np.log(np.maximum(hsrl_profs[ch]['fit'],0.001)))\n",
    "\n",
    "print('negative LL fixed kernel')\n",
    "print(negLL_fixed)\n",
    "print('negative LL optimized')\n",
    "print(negLL_optimized)\n",
    "print('negative LL unfiltered')\n",
    "print(negLL_unfiltered)\n",
    "\n",
    "print()\n",
    "print('negative LL fixed kernel - optimized')\n",
    "print(negLL_fixed-negLL_optimized)\n",
    "print('negative LL unfiltered kernel - optimized')\n",
    "print(negLL_unfiltered-negLL_optimized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the smoothing where the kernels are optimized to the scene produces the best validation with the test data.  Of the three cases we evaluated, it provides the best estimate of the lidar backscatter signal.\n",
    "\n",
    "The plots below show the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_data = np.array([negLL_unfiltered,negLL_optimized,negLL_fixed,])\n",
    "bar_data2 = np.array([negLL_unfiltered-negLL_optimized,negLL_fixed-negLL_optimized,])\n",
    "bar_data = bar_data-np.mean(bar_data)\n",
    "plt.figure()\n",
    "plt.bar(np.arange(bar_data.size),bar_data,tick_label=['unfiltered','optimized','fixed'])\n",
    "plt.ylabel('Negative Log-Likelihood\\n(Difference from Mean)')\n",
    "plt.grid(b=True)\n",
    "\n",
    "\n",
    "# difference in negative log-likelihood of two fixed cases and the optimized case\n",
    "plt.figure()\n",
    "plt.bar(np.arange(bar_data2.size),bar_data2,tick_label=['unfiltered','fixed'])\n",
    "plt.ylabel('Negative Log-Likelihood Difference')\n",
    "plt.grid(b=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the comparison plot used in the publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = 'Combined_Counts'\n",
    "plt.figure()\n",
    "ax1 = plt.subplot2grid((6, 16), (0, 0), colspan=8,rowspan=2)\n",
    "cimg = ax1.pcolor(time,hsrl_lidar_range*1e-3,(hsrl_fixedfilt_profs[ch]-hsrl_profs[ch]['bg']).T,norm=LogNorm(),vmin=1,vmax=1e3)\n",
    "ax1.set_ylabel('Altitude [km]',fontsize=7)\n",
    "ax1.get_xaxis().set_visible(False)\n",
    "ax1.tick_params(axis='y', labelsize=7)\n",
    "\n",
    "\n",
    "ax2 = plt.subplot2grid((6, 16), (2, 0), colspan=8,rowspan=2)\n",
    "ax2.pcolor(time,hsrl_lidar_range*1e-3,(hsrl_profs[ch]['filtered']-hsrl_profs[ch]['bg']).T,norm=LogNorm(),vmin=1,vmax=1e3)\n",
    "ax2.set_ylabel('Altitude [km]',fontsize=7)\n",
    "ax2.get_xaxis().set_visible(False)\n",
    "ax2.tick_params(axis='y', labelsize=7)\n",
    "\n",
    "\n",
    "ax4 = plt.subplot2grid((6, 16), (4, 0), colspan=8,rowspan=1)\n",
    "ax4.plot(time,range_filt_size[isol[ch]['range']]*dz,'.',markersize=2,alpha=0.5,label=ch.replace('_Counts',' '))\n",
    "ax4.plot(time,filter_width_r*dz*np.ones(time.size),':k',label=ch.replace('_Counts',' '))\n",
    "ax4.set_xlabel('Time [UTC]',fontsize=7)\n",
    "ax4.set_ylabel('Filter\\nWidth [m]',fontsize=7)\n",
    "ax4.grid(b=True,which='both',axis='y')\n",
    "ax4.grid(b=True,which='major',axis='x')\n",
    "ax4.tick_params(axis='x', labelsize=7)\n",
    "ax4.tick_params(axis='y', labelsize=7)\n",
    "ax4.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "ax4.set_xlim(ax1.get_xlim())\n",
    "ax4.set_yscale('log')\n",
    "\n",
    "\n",
    "ax3 = plt.subplot2grid((6, 16), (2, 8), rowspan=2, colspan=3)\n",
    "ax3.plot(time_filt_size[isol[ch]['time']]*dt,hsrl_lidar_range*1e-3,'.',markersize=2,alpha=0.5,label=ch.replace('_Counts',' '))\n",
    "ax3.plot(filter_width_t*dt*np.ones(hsrl_lidar_range.size),hsrl_lidar_range*1e-3,':k',label=ch.replace('_Counts',' '))\n",
    "ax3.set_xlabel('Filter\\nWidth [s]',fontsize=7)\n",
    "labels = [item.get_text() for item in ax3.get_yticklabels()]\n",
    "empty_string_labels = ['']*len(labels)\n",
    "ax3.set_yticklabels(empty_string_labels)\n",
    "ax3.tick_params(axis='x', labelsize=7)\n",
    "#ax3.get_yaxis().set_visible(False)\n",
    "ax3.set_ylim(ax2.get_ylim())\n",
    "ax3.set_xscale('log')\n",
    "ax3.xaxis.set_major_locator(ticker.LogLocator(base=10.0,numticks=15))\n",
    "\n",
    "\n",
    "ax3.grid(b=True,which='both',axis='x')\n",
    "ax3.grid(b=True,which='major',axis='y')\n",
    "\n",
    "cax = plt.subplot2grid((6, 8), (0, 4), rowspan=2, colspan=1)\n",
    "cb = plt.colorbar(cimg, cax = cax)\n",
    "cb.set_label('Photon Counts',fontsize=7)\n",
    "cb.ax.tick_params(labelsize=7) \n",
    "\n",
    "# plt.savefig('../../plots/Optimized_2D_Filter_'+ch+'3_v2.png',dpi=300,bbox_inches='tight')\n",
    "# plt.savefig('../../plots/Optimized_2D_Filter_'+ch+'3_v2.tif',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform a simplified calculation of the backscatter ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate the backscatter ratio for the difference cases\n",
    "\"\"\"\n",
    "\n",
    "mol_gain = 2.0\n",
    "backscatter_ratio_ver = (hsrl_profs['Combined_Counts']['ver']-hsrl_profs['Combined_Counts']['bg'])/ \\\n",
    "                        (hsrl_profs['Molecular_Counts']['ver']-hsrl_profs['Molecular_Counts']['bg'])/mol_gain\n",
    "backscatter_ratio = (hsrl_profs['Combined_Counts']['fit']-hsrl_profs['Combined_Counts']['bg'])/ \\\n",
    "                        (hsrl_profs['Molecular_Counts']['fit']-hsrl_profs['Molecular_Counts']['bg'])/mol_gain\n",
    "backscatter_ratio_filtered = (hsrl_profs['Combined_Counts']['filtered']-hsrl_profs['Combined_Counts']['bg'])/ \\\n",
    "                        (hsrl_profs['Molecular_Counts']['filtered']-hsrl_profs['Molecular_Counts']['bg'])/mol_gain\n",
    "\n",
    "backscatter_ratio_fixedfilt = (hsrl_fixedfilt_profs['Combined_Counts']-hsrl_profs['Combined_Counts']['bg'])/ \\\n",
    "                        (hsrl_fixedfilt_profs['Molecular_Counts']-hsrl_profs['Molecular_Counts']['bg'])/mol_gain    \n",
    "    \n",
    "backscatter_ratio_overfilt = (hsrl_profs['Combined_Counts']['overfilt']-hsrl_profs['Combined_Counts']['bg'])/ \\\n",
    "                        (hsrl_profs['Molecular_Counts']['overfilt']-hsrl_profs['Molecular_Counts']['bg'])/mol_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(backscatter_ratio[:,::-1].T,norm=LogNorm())\n",
    "plt.title('Unfiltered')\n",
    "plt.clim(1,1e2)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(backscatter_ratio_filtered[:,::-1].T,norm=LogNorm())\n",
    "plt.title('Optimized Filters')\n",
    "plt.clim(1,1e2)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(backscatter_ratio_fixedfilt[:,::-1].T,norm=LogNorm())\n",
    "plt.title('Fixed Filter')\n",
    "plt.clim(1,1e2)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(backscatter_ratio_overfilt[:,::-1].T,norm=LogNorm())\n",
    "plt.clim(1,1e2)\n",
    "plt.title('Over Filtered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above compare the cases where backscatter ratio is estimated with no smoothing (top), optimized smoothing in time and range (middle), a fixed filter size (based on Hayman and Spuler 2017 [4]) and optimized temporal smoothing with over smoothing in range (bottom).  The top and bottom figures show the extremes of error contributions from stochastic noise (top) and smearing (bottom).  The fixed kernel is an improvement over the extreme cases but does not capture the high altitude clouds as well as the optimized kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "ax.plot(backscatter_ratio[itime,:],hsrl_lidar_range,'.-',alpha=0.5,label='unfiltered')\n",
    "ax.plot(backscatter_ratio_filtered[itime,:],hsrl_lidar_range,'.-',label='optimized')\n",
    "ax.plot(backscatter_ratio_fixedfilt[itime,:],hsrl_lidar_range,'.-',label='fixed')\n",
    "ax.plot(backscatter_ratio_overfilt[itime,:],hsrl_lidar_range,'.-',alpha=0.5,label='over filtered')\n",
    "ax.set_ylim([0,10e3])\n",
    "ax.set_xlim([1,100])\n",
    "ax.set_xscale('log')\n",
    "ax.grid(b=True)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax1 = plt.subplot2grid((3, 3), (0, 0), colspan=2,rowspan=2)\n",
    "ax1.pcolor(time,hsrl_lidar_range*1e-3,backscatter_ratio_filtered.T,norm=LogNorm(),vmin=1,vmax=1e2)\n",
    "ax1.set_ylabel('Altitude [km]')\n",
    "ax1.set_title('Backscatter Ratio')\n",
    "ax1.get_xaxis().set_visible(False)\n",
    "ax1.tick_params(axis='y', labelsize=7)\n",
    "\n",
    "ax2 = plt.subplot2grid((3, 3), (2, 0), colspan=2)\n",
    "for ch in hsrl_profs:\n",
    "    ax2.plot(time,range_filt_size[isol[ch]['range']]*dz,'.',markersize=2,alpha=0.5,label=ch.replace('_Counts',' '))\n",
    "ax2.set_xlabel('Time [UTC]')\n",
    "ax2.set_ylabel('Range Resolution [m]')\n",
    "ax2.grid(b=True,which='both',axis='y')\n",
    "ax2.grid(b=True,which='major',axis='x')\n",
    "ax2.tick_params(axis='x', labelsize=7)\n",
    "ax2.tick_params(axis='y', labelsize=7)\n",
    "ax2.format_xdata = mdates.DateFormatter('%H:%M')\n",
    "\n",
    "\n",
    "ax3 = plt.subplot2grid((3, 3), (0, 2), rowspan=2)\n",
    "for ch in hsrl_profs:\n",
    "    ax3.plot(time_filt_size[isol[ch]['time']]*dt,hsrl_lidar_range,'.',markersize=2,alpha=0.5,label=ch.replace('_Counts',' '))\n",
    "ax3.set_xlabel('Time Resolution [s]')\n",
    "labels = [item.get_text() for item in ax3.get_yticklabels()]\n",
    "empty_string_labels = ['']*len(labels)\n",
    "ax3.set_yticklabels(empty_string_labels)\n",
    "\n",
    "ax3.grid(b=True,which='both',axis='x')\n",
    "ax3.grid(b=True,which='major',axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "02b24369-ab1a-4f0b-9d45-538d8c36357f",
    "theme": {}
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
