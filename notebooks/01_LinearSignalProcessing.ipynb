{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of linear signal processing in photon counting lidar\n",
    "This notebook was developed as a suppliment to the publication\n",
    "Optimization of photon counting lidar signal processing through Poisson thinning\n",
    "by Matthew Hayman\n",
    "submitted to Optics Letters *In Review*, DOI:TBD\n",
    "\n",
    "The notebook demonstrates the basic concepts for optimizing linear smoothing\n",
    "kernenls to a particular lidar scene by splitting an observed photon count\n",
    "profile into a \"fit\" and \"verification\" profile.  Smoothing kernels are applied\n",
    "to the \"fit\" profile then evaluated against the verification data set to find \n",
    "the optimal filter kernel.\n",
    "\n",
    "The code and functions contained in this notebook are available for public use\n",
    "so long as the original publication is referenced in any published work\n",
    "\n",
    "The data used in this example is from an NCAR MicroPulse DIAL (MPD) [1] using the potassium HSRL channels [2].  This \n",
    "data is provided for example purposes only and should not be used in scientific\n",
    "study.  No data quality assurance can be provided for this dataset.\n",
    "\n",
    "1. Spuler et al., \"Field-deployable diode-laser-based differential absorption lidar (DIAL) for profiling water vapor,\" Atmos. Meas. Tech., 8, 1073â€“1087, DOI:10.5194/amt-8-1073-2015 (2015).\n",
    "\n",
    "2. Stillewell et al.,\"Demonstration of a combined differential absorption and high spectral resolution lidar for profiling atmospheric temperature,\" Opt. Express, 28, 71-93, DOI: 10.1364/OE.379804 (2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "fc80df9c-6bde-4141-8896-29bcc42292b9"
    }
   },
   "outputs": [],
   "source": [
    "# import python libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "94399d8d-a6a8-458e-9ba7-0c1462efd429"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: set relative path\n",
    "# os.path.abspath('../../')\n",
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "40194af8-85a4-4216-8510-ea17fbaa57b1"
    }
   },
   "outputs": [],
   "source": [
    "ncfile = 'mpd05.20181022T12300019921_20181022T15163019921.nc'\n",
    "# ncfile = 'mpd05.20181022T18032019921_20181022T20495019531.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of lidar profile names to be loaded\n",
    "lidar_profile_data = ['Combined_Counts','Molecular_Counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "0e595ff4-fda4-472b-9f61-514ce4a0c825"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function definitions for optimizing the filter\n",
    "\"\"\"\n",
    "\n",
    "def poisson_thin(pc_data):\n",
    "    \"\"\"\n",
    "    Poisson thin photon counting data\n",
    "    \n",
    "    inputs:\n",
    "        pc_data - array of raw photon count data\n",
    "            This is assumed to have a Poisson PDF\n",
    "    outputs:\n",
    "        pc_1,pc_2 - the resulting thinned photon count\n",
    "            arrays\n",
    "    \"\"\"\n",
    "    # the first thinned profile is calculated using a binomial random\n",
    "    # number generator to simulate \n",
    "    # flipping a coin to decide if a photon count is included in this\n",
    "    # profile or not\n",
    "    pc_1 = np.random.binomial(pc_data.astype(np.int),0.5,size=pc_data.shape)\n",
    "    # the second profile is whatever photon counts are left\n",
    "    pc_2 = pc_data-pc_1\n",
    "    \n",
    "    return pc_1,pc_2\n",
    "\n",
    "def build_Gaussian_kernel(sigt,sigz,norm=True):\n",
    "    \"\"\"\n",
    "    Generates a Gaussian convolution kernel for\n",
    "    standard deviations sigt and sigz in units of grid points.\n",
    "    sigt and sigz are defined in units of grid steps\n",
    "    \"\"\"        \n",
    "\n",
    "    \n",
    "    nt = np.round(4*sigt) # estimate size of time grid needed\n",
    "    nz = np.round(4*sigz) # estimate size of range grid needed\n",
    "    t = np.arange(-nt,nt+1) # create time grid     \n",
    "    z = np.arange(-nz,nz+1) # create range grid\n",
    "    \n",
    "    \n",
    "    # build Gaussian kernel in time\n",
    "    kconv_t = np.exp(-t**2*1.0/(sigt**2))\n",
    "    \n",
    "    # check for singularities in the definition\n",
    "    # if they exist, make the filter a delta function in time\n",
    "    if kconv_t.size > 1:\n",
    "        if np.sum(kconv_t) == 0:\n",
    "            it0 = np.argmin(np.abs(t))\n",
    "            kconv_t[it0] = 1.0\n",
    "    else: \n",
    "        kconv_t = np.ones(1)\n",
    "\n",
    "    # build Gaussian kernel in range\n",
    "    kconv_z = np.exp(-z**2*1.0/(sigz**2))\n",
    "    \n",
    "    # check for singularities in the definition\n",
    "    # if they exist, make the filter a delta function in range\n",
    "    if kconv_z.size > 1:\n",
    "        if np.sum(kconv_z) == 0:\n",
    "            iz0 = np.argmin(np.abs(z))\n",
    "            kconv_z[iz0] = 1.0\n",
    "    else:\n",
    "        kconv_z = np.ones(1)\n",
    "\n",
    "    # combine the time and range kernels\n",
    "    kconv = kconv_t[:,np.newaxis]*kconv_z[np.newaxis,:]\n",
    "\n",
    "    # normalize the area of the kernel to conserve energy\n",
    "    if norm:\n",
    "        kconv = kconv/(1.0*np.sum(kconv))\n",
    "\n",
    "    return kconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "5d0a1103-080a-4dec-b260-2383b14c92de"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the photon counts from the netcdf file\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "profs = {}\n",
    "with xr.open_dataset(data_path+ncfile) as ds:\n",
    "    for lvar in lidar_profile_data:\n",
    "        profs[lvar] = ds[lvar].values\n",
    "    lidar_range = ds['range'].values.copy()\n",
    "    time = ds['time'].values.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook will run a demonstration case on the combined\n",
    "channel of the MPD HSRL operating at 770.  This is effectively\n",
    "equivilant to a backscatter lidar observation.\n",
    "\"\"\"\n",
    "demo_var = 'Combined_Counts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variety of features in the scene from this data file.  Thick low clouds are between time indices 1000 and 2000.  High ice clouds are seen intermittantly starting around time index 2500 until lower clouds arrive late in the day near time index 7500.  The sun rises around time index 4800 and several instances of high background occur due to the simultaneous presence of sun and clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a quicklook of the backscatter data\n",
    "fig,ax = plt.subplots(1,1,figsize=(18,4))\n",
    "ax.imshow(profs[demo_var][:,::-1].T,norm=LogNorm())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demonstration presented here focuses on optimization of range smoothing kernels, but kernels can similarly be optimized for time smoothing or 2D kernels consisting of range and time components.  \n",
    "\n",
    "Generally we perform both time and range optimization independently where each time bin has its own unique smoothing kernel and each range bin has its own uniquely determined time smoothing kernel.\n",
    "\n",
    "In order to compare methodology, thinning is performed once and different methods are subsequently applied to that same data.  Comparing results between two different thinned cases, even if they originate from the same profile is generally not effective because the offset in the inverse log-likelihood is prone to variation between different thinning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "45b877c2-1d2c-4de5-b6e5-01e1eeb13ee1"
    }
   },
   "outputs": [],
   "source": [
    "itime =  400 # select a profile index for this demonstration case \n",
    "              # interesting cases: \n",
    "              #   night light clouds: 3500\n",
    "              #   daytime/high background medium clouds: 5500\n",
    "              #   dattime/high background dense clouds: 7500\n",
    "              #   daytime/high background, low alt aerosols, thin cirrus:  7000\n",
    "\n",
    "max_alt = 12e3\n",
    "\n",
    "imax_alt = np.argmin(np.abs(lidar_range-max_alt))\n",
    "            \n",
    "# assign the demonstration case to its own independent variable\n",
    "pdemo = (profs[demo_var][itime,3:imax_alt])[np.newaxis,:]  # remove bottom three bins from laser \"bang\"\n",
    "plidar_range = lidar_range[3:imax_alt]  # create a new range array for the bang-removed profile\n",
    "dz = np.mean(np.diff(plidar_range))  # store the range resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson Thinning\n",
    "Poisson thinning generates two statistically independent observations from one observation.  The resulting thinned profiles have the same underlying mean photon arrival rate driving the observations.  This is demonstrated below on the two thinned profiles are\n",
    "```\n",
    "pfit\n",
    "```\n",
    "used for applying the filter ($\\mathbf{f}$ in the publication) and \n",
    "```\n",
    "pver\n",
    "```\n",
    "used for scoring the the filtered result ($\\mathbf{g}$ in the publication).\n",
    "\n",
    "If the two observations have equal signals but uncorrelated noise, then\n",
    "$$E\\left[\\frac{\\mathbf{f}-\\mathbf{g}}{\\sqrt{\\sigma_f^2+\\sigma_g^2}}\\right] \\approx 0 $$\n",
    "and \n",
    "$$std\\left[\\frac{\\mathbf{f}-\\mathbf{g}}{\\sqrt{\\sigma_f^2+\\sigma_g^2}}\\right] \\approx 1 $$\n",
    "\n",
    "where for a Poisson observation $f$, the variance in the estimate of mean photons is given by $\\sigma_f^2 = f+1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poisson thin the raw photon counts of the profile\n",
    "pfit,pver = poisson_thin(pdemo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "931a31a7-63c4-415a-b84b-588344976d7b"
    }
   },
   "outputs": [],
   "source": [
    "# plot the thinned data\n",
    "# the two verify and fit data should have \n",
    "# statistically independent noise but common \n",
    "# underlying signals\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "ax.plot(pdemo.flatten(),plidar_range,'k.',label='observation')\n",
    "ax.plot(pfit.flatten(),plidar_range,'.',alpha=0.5,label='fit data')\n",
    "ax.plot(pver.flatten(),plidar_range,'.',alpha=0.5,label='verify data')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('Range [m]')\n",
    "ax.set_xlabel('Photon Counts')\n",
    "ax.grid(b=True)\n",
    "ax.set_ylim([0,12e3])\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original photon count data, thinned fit data, and thinned verification data are shown above.  The shapes of the thinned profiles are the same but the statistical noise is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the difference in photon counts between the\n",
    "# fit and verify profiles \n",
    "# compare this to the expected standard deviation\n",
    "# for uncorrelated Poisson observations\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "ax.plot(pfit.flatten()-pver.flatten(),plidar_range,'g.',label='Actual')\n",
    "ax.plot(np.sqrt(pfit+1+pver+1).flatten(),plidar_range,'k--',label='Expected std.')\n",
    "ax.plot(-np.sqrt(pfit+1+pver+1).flatten(),plidar_range,'k--')\n",
    "ax.set_ylabel('Range [m]')\n",
    "ax.set_xlabel('Photon Count Difference')\n",
    "ax.grid(b=True)\n",
    "ax.set_ylim([0,12e3])\n",
    "ax.legend()\n",
    "\n",
    "# adjusting by the expected uncertainty results in a\n",
    "# a mean value near zero with\n",
    "# a standard deviation near one\n",
    "# further suggesting the signals are common but the\n",
    "# noise is statistically independent\n",
    "print('mean: %f'%np.mean((pfit-pver)/np.sqrt(pfit+1+pver+1)))\n",
    "print('std: %f'%np.std((pfit-pver)/np.sqrt(pfit+1+pver+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the difference between the two thinned profiles (green dots) with the theoretically estimated variance in the difference assuming both profiles are observations of Poisson random variables and they are uncorrelated. ($\\sqrt{\\sigma_f^2+\\sigma_g^2}$).  The theoretically estimated standard deviation appears representative of the scatter in the difference signal and the scattered signal appears to be zero mean.\n",
    "\n",
    "The standard deviation normalized mean and standard deviation are reported above these figures.  The mean is near zero and the standard deviation is near one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Optimal Filter\n",
    "The optimal filter kernel in a set is found by applying all the kernels to $\\mathbf{f}$ then scoring the result $\\tilde{\\mathbf{f}}$ against the verification profile $\\mathbf{g}$.  The score is calculated using the inverse log-likelihood\n",
    "$$\\mathcal{E} = \\tilde{\\mathbf{f}} - \\mathbf{g}\\ln \\tilde{\\mathbf{f}} $$\n",
    "The filter that produces the lowest inverse log-likelihood is taken to be the best filter from the set for this particular scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "040adc1c-37dd-46ec-a4d1-5f3554cd1d50"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Filter the raw profile\n",
    "\"\"\"\n",
    "\n",
    "# define range of filters to try\n",
    "filt_size = np.logspace(-1,1,40)\n",
    "\n",
    "# initialize the output arrays\n",
    "inv_ll = np.zeros(filt_size.size)\n",
    "filt_profs = np.zeros((filt_size.size,pfit.size))\n",
    "\n",
    "for ifilt,filter_width in enumerate(filt_size):\n",
    "    # get the convolution kernel\n",
    "    kern = build_Gaussian_kernel(0,filter_width)\n",
    "    \n",
    "    # normalize by the amount of points in a region\n",
    "    # to avoid edge effects\n",
    "    norm = np.ones(pfit.shape)\n",
    "    norm = scipy.signal.convolve2d(norm,kern,mode='same')\n",
    "    # apply the filter and normalize the result\n",
    "    # with the amount of data points contributing\n",
    "    pfilt = scipy.signal.convolve2d(pfit,kern,mode='same')/norm\n",
    "    \n",
    "    # save the results\n",
    "    filt_profs[ifilt,:] = pfilt\n",
    "    inv_ll[ifilt] = np.nansum(pfilt-pver*np.log(pfilt))\n",
    "\n",
    "# get the index to the solution\n",
    "isol = np.argmin(inv_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "ad9131dc-6483-47ee-9768-fbf810d7aafb"
    }
   },
   "outputs": [],
   "source": [
    "# plot the resulting optimized profile over the raw data\n",
    "# also include an over-filtered example\n",
    "# in a separate plot show the inverse log-likelihood as a function of filter width\n",
    "\n",
    "fig,ax = plt.subplots(2,1,figsize=(5,10))\n",
    "ax[0].plot(pver.flatten(),plidar_range,'.',label='raw data')\n",
    "ax[0].plot(filt_profs[isol,:],plidar_range,'k--',label='filtered')\n",
    "ax[0].plot(filt_profs[-1,:],plidar_range,':',label='over filtered')\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_ylabel('Range [m]')\n",
    "ax[0].set_xlabel('Photon Counts')\n",
    "ax[0].grid(b=True)\n",
    "ax[0].set_ylim([0,12e3])\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(filt_size*dz,inv_ll,'b.')\n",
    "ax[1].plot(filt_size[isol]*dz,inv_ll[isol],'gs')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].grid(b=True)\n",
    "ax[1].set_xlabel('Filter Width [m]')\n",
    "ax[1].set_ylabel('Inverse Log-Likelihood')\n",
    "\n",
    "#ax[1].set_ylim([inv_ll.min()*1.1,0.01*np.median(inv_ll)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top figure shows the verification data in blue dots and the optimally filtered data is in the blacked dashed line.  For comparison, an over filtered case is also shown as the orange dotted line.  Overfiltering is more effective as supressing the random errors, but also skews the signal, particularly where clouds are present.\n",
    "\n",
    "The bottom figure shows the inverse log-likelihood for each filter kernel width (defined by the standard deviation of a Gaussian) where the minimum (optimal) value is indicated by the green square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Optimization for Backscatter Ratio\n",
    "Principles described above can be similarly applied to multiple channels in two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_alt = 12e3\n",
    "\n",
    "imax_alt = np.argmin(np.abs(lidar_range-max_alt))\n",
    "\n",
    "hsrl_profs = {}\n",
    "for var in lidar_profile_data:\n",
    "    # assign the demonstration case to its own independent variable\n",
    "    ptemp = profs[var]      # remove bottom three bins from laser \"bang\"\n",
    "    hsrl_profs[var] = {}\n",
    "    hsrl_profs[var]['raw'] = ptemp.copy()\n",
    "    hsrl_profs[var]['bg'] = 0.5*np.mean(hsrl_profs[var]['raw'][:,-100:],axis=1)[:,np.newaxis]\n",
    "    \n",
    "    hsrl_profs[var]['raw']=hsrl_profs[var]['raw'][:,3:imax_alt]\n",
    "    \n",
    "    # Poisson thin the raw photon counts of the profile\n",
    "    hsrl_profs[var]['fit'],hsrl_profs[var]['ver'] = poisson_thin(hsrl_profs[var]['raw'])\n",
    "    \n",
    "hsrl_lidar_range = lidar_range[3:imax_alt]  # create a new range array for the bang-removed profile\n",
    "dz = np.mean(np.diff(hsrl_lidar_range))  # store the range resolution  \n",
    "dt = np.mean(np.diff(time))  # store the time resolution  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backscatter_ratio = (hsrl_profs['Combined_Counts']['fit']-hsrl_profs['Combined_Counts']['bg'])/ \\\n",
    "                        (hsrl_profs['Molecular_Counts']['fit']-hsrl_profs['Molecular_Counts']['bg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(backscatter_ratio[:,::-1].T,norm=LogNorm())\n",
    "plt.clim([1,1e3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "02b24369-ab1a-4f0b-9d45-538d8c36357f",
    "theme": {}
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
