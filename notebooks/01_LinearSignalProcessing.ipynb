{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of linear signal processing in photon counting lidar\n",
    "This notebook was developed as a suppliment to the publication\n",
    "\n",
    "Matthew Hayman, Robert A. Stillwell and Scott M. Spuler, \n",
    "\"Optimization of linear photon counting lidar signal processing through Poisson thinning,\"\n",
    "submitted to Optics Letters *In Review*, DOI:TBD\n",
    "\n",
    "The notebook demonstrates the basic concepts for optimizing linear smoothing\n",
    "kernenls to a particular lidar scene by splitting an observed photon count\n",
    "profile into a \"fit\" and \"verification\" profile.  Smoothing kernels are applied\n",
    "to the \"fit\" profile then evaluated against the verification data set to find \n",
    "the optimal filter kernel.\n",
    "\n",
    "The code and functions contained in this notebook are available for public use\n",
    "so long as the original publication is referenced in any published work\n",
    "\n",
    "The data used in this example is from an NCAR MicroPulse DIAL (MPD) [1] using the potassium HSRL channels [2].  This \n",
    "data is provided for example purposes only and should not be used in scientific\n",
    "study.  No data quality assurance can be provided for these datasets.\n",
    "\n",
    "1. Spuler et al., \"Field-deployable diode-laser-based differential absorption lidar (DIAL) for profiling water vapor,\" Atmos. Meas. Tech., 8, 1073â€“1087, DOI:10.5194/amt-8-1073-2015 (2015).\n",
    "\n",
    "2. Stillewell et al.,\"Demonstration of a combined differential absorption and high spectral resolution lidar for profiling atmospheric temperature,\" Opt. Express, 28, 71-93, DOI: 10.1364/OE.379804 (2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "fc80df9c-6bde-4141-8896-29bcc42292b9"
    }
   },
   "outputs": [],
   "source": [
    "# import python libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "94399d8d-a6a8-458e-9ba7-0c1462efd429"
    }
   },
   "outputs": [],
   "source": [
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "40194af8-85a4-4216-8510-ea17fbaa57b1"
    }
   },
   "outputs": [],
   "source": [
    "ncfile = 'mpd05.20181022T12300019921_20181022T15163019921.nc'\n",
    "# ncfile = 'mpd05.20181022T18032019921_20181022T20495019531.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of lidar profile names to be loaded\n",
    "lidar_profile_data = ['Combined_Counts','Molecular_Counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "0e595ff4-fda4-472b-9f61-514ce4a0c825"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function definitions for optimizing the filter\n",
    "\"\"\"\n",
    "\n",
    "def poisson_thin(pc_data):\n",
    "    \"\"\"\n",
    "    Poisson thin photon counting data\n",
    "    \n",
    "    inputs:\n",
    "        pc_data - array of raw photon count data\n",
    "            This is assumed to have a Poisson PDF\n",
    "    outputs:\n",
    "        pc_1,pc_2 - the resulting thinned photon count\n",
    "            arrays\n",
    "    \"\"\"\n",
    "    # the first thinned profile is calculated using a binomial random\n",
    "    # number generator to simulate \n",
    "    # flipping a coin to decide if a photon count is included in this\n",
    "    # profile or not\n",
    "    pc_1 = np.random.binomial(pc_data.astype(np.int),0.5,size=pc_data.shape)\n",
    "    # the second profile is whatever photon counts are left\n",
    "    pc_2 = pc_data-pc_1\n",
    "    \n",
    "    return pc_1,pc_2\n",
    "\n",
    "def build_Gaussian_kernel(sigt,sigz,norm=True):\n",
    "    \"\"\"\n",
    "    Generates a Gaussian convolution kernel for\n",
    "    standard deviations sigt and sigz in units of grid points.\n",
    "    sigt and sigz are defined in units of grid steps\n",
    "    \"\"\"        \n",
    "\n",
    "    \n",
    "    nt = np.round(4*sigt) # estimate size of time grid needed\n",
    "    nz = np.round(4*sigz) # estimate size of range grid needed\n",
    "    t = np.arange(-nt,nt+1) # create time grid     \n",
    "    z = np.arange(-nz,nz+1) # create range grid\n",
    "    \n",
    "    \n",
    "    # build Gaussian kernel in time\n",
    "    kconv_t = np.exp(-t**2*1.0/(sigt**2))\n",
    "    \n",
    "    # check for singularities in the definition\n",
    "    # if they exist, make the filter a delta function in time\n",
    "    if kconv_t.size > 1:\n",
    "        if np.sum(kconv_t) == 0:\n",
    "            it0 = np.argmin(np.abs(t))\n",
    "            kconv_t[it0] = 1.0\n",
    "    else: \n",
    "        kconv_t = np.ones(1)\n",
    "\n",
    "    # build Gaussian kernel in range\n",
    "    kconv_z = np.exp(-z**2*1.0/(sigz**2))\n",
    "    \n",
    "    # check for singularities in the definition\n",
    "    # if they exist, make the filter a delta function in range\n",
    "    if kconv_z.size > 1:\n",
    "        if np.sum(kconv_z) == 0:\n",
    "            iz0 = np.argmin(np.abs(z))\n",
    "            kconv_z[iz0] = 1.0\n",
    "    else:\n",
    "        kconv_z = np.ones(1)\n",
    "\n",
    "    # combine the time and range kernels\n",
    "    kconv = kconv_t[:,np.newaxis]*kconv_z[np.newaxis,:]\n",
    "\n",
    "    # normalize the area of the kernel to conserve energy\n",
    "    if norm:\n",
    "        kconv = kconv/(1.0*np.sum(kconv))\n",
    "\n",
    "    return kconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "5d0a1103-080a-4dec-b260-2383b14c92de"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the photon counts from the netcdf file\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "profs = {}\n",
    "with xr.open_dataset(data_path+ncfile) as ds:\n",
    "    for lvar in lidar_profile_data:\n",
    "        profs[lvar] = ds[lvar].values\n",
    "    lidar_range = ds['range'].values.copy()\n",
    "    time = ds['time'].values.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook will run a demonstration case on the combined\n",
    "channel of the MPD HSRL operating at 770.  This is effectively\n",
    "equivilant to a backscatter lidar observation.\n",
    "\"\"\"\n",
    "demo_var = 'Combined_Counts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variety of features in the scene from this data file.  Thick low clouds are between time indices 1000 and 2000.  High ice clouds are seen intermittantly starting around time index 2500 until lower clouds arrive late in the day near time index 7500.  The sun rises around time index 4800 and several instances of high background occur due to the simultaneous presence of sun and clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a quicklook of the backscatter data\n",
    "fig,ax = plt.subplots(1,1,figsize=(18,4))\n",
    "ax.imshow(profs[demo_var][:,::-1].T,norm=LogNorm())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demonstration presented here focuses on optimization of range smoothing kernels, but kernels can similarly be optimized for time smoothing or 2D kernels consisting of range and time components.  \n",
    "\n",
    "Generally we perform both time and range optimization independently where each time bin has its own unique smoothing kernel and each range bin has its own uniquely determined time smoothing kernel.\n",
    "\n",
    "In order to compare methodology, thinning is performed once and different methods are subsequently applied to that same data.  Comparing results between two different thinned cases, even if they originate from the same profile is generally not effective because the offset in the inverse log-likelihood is prone to variation between different thinning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "45b877c2-1d2c-4de5-b6e5-01e1eeb13ee1"
    }
   },
   "outputs": [],
   "source": [
    "itime = 400  # select a profile index for this demonstration case \n",
    "              # interesting cases: \n",
    "              #   thin high cloud: 400\n",
    "              #   thick cirrus: 150\n",
    "\n",
    "\n",
    "max_alt = 12e3\n",
    "\n",
    "imax_alt = np.argmin(np.abs(lidar_range-max_alt))\n",
    "            \n",
    "# assign the demonstration case to its own independent variable\n",
    "pdemo = (profs[demo_var][itime,3:imax_alt])[np.newaxis,:]  # remove bottom three bins from laser \"bang\"\n",
    "plidar_range = lidar_range[3:imax_alt]  # create a new range array for the bang-removed profile\n",
    "dz = np.mean(np.diff(plidar_range))  # store the range resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson Thinning\n",
    "Poisson thinning generates two statistically independent observations from one observation.  The resulting thinned profiles have the same underlying mean photon arrival rate driving the observations.  This is demonstrated below on the two thinned profiles are\n",
    "```\n",
    "pfit\n",
    "```\n",
    "used for applying the filter ($\\mathbf{f}$ in the publication) and \n",
    "```\n",
    "pver\n",
    "```\n",
    "used for scoring the the filtered result ($\\mathbf{g}$ in the publication).\n",
    "\n",
    "If the two observations have equal signals but uncorrelated noise, then\n",
    "$$E\\left[\\frac{\\mathbf{f}-\\mathbf{g}}{\\sqrt{\\sigma_f^2+\\sigma_g^2}}\\right] \\approx 0 $$\n",
    "and \n",
    "$$std\\left[\\frac{\\mathbf{f}-\\mathbf{g}}{\\sqrt{\\sigma_f^2+\\sigma_g^2}}\\right] \\approx 1 $$\n",
    "\n",
    "where for a Poisson observation $f$, the variance in the estimate of mean photons is given by $\\sigma_f^2 = f+1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poisson thin the raw photon counts of the profile\n",
    "pfit,pver = poisson_thin(pdemo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "931a31a7-63c4-415a-b84b-588344976d7b"
    }
   },
   "outputs": [],
   "source": [
    "# plot the thinned data\n",
    "# the two verify and fit data should have \n",
    "# statistically independent noise but common \n",
    "# underlying signals\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "ax.plot(pdemo.flatten(),plidar_range*1e-3,'k.',label='observation')\n",
    "ax.plot(pfit.flatten(),plidar_range*1e-3,'.',alpha=0.5,label='fit data')\n",
    "ax.plot(pver.flatten(),plidar_range*1e-3,'.',alpha=0.5,label='verify data')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('Range [km]')\n",
    "ax.set_xlabel('Photon Counts')\n",
    "ax.grid(b=True)\n",
    "ax.set_ylim([0,12])\n",
    "ax.legend(loc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original photon count data, thinned fit data, and thinned verification data are shown above.  The shapes of the thinned profiles are the same but the statistical noise is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the difference in photon counts between the\n",
    "# fit and verify profiles \n",
    "# compare this to the expected standard deviation\n",
    "# for uncorrelated Poisson observations\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "ax.plot(pfit.flatten()-pver.flatten(),plidar_range,'g.',label='Actual')\n",
    "ax.plot(np.sqrt(pfit+1+pver+1).flatten(),plidar_range,'k--',label='Expected std.')\n",
    "ax.plot(-np.sqrt(pfit+1+pver+1).flatten(),plidar_range,'k--')\n",
    "ax.set_ylabel('Range [m]')\n",
    "ax.set_xlabel('Photon Count Difference')\n",
    "ax.grid(b=True)\n",
    "ax.set_ylim([0,12e3])\n",
    "ax.legend(loc=2)\n",
    "\n",
    "# adjusting by the expected uncertainty results in a\n",
    "# a mean value near zero with\n",
    "# a standard deviation near one\n",
    "# further suggesting the signals are common but the\n",
    "# noise is statistically independent\n",
    "print('mean: %f'%np.mean((pfit-pver)/np.sqrt(pfit+1+pver+1)))\n",
    "print('std: %f'%np.std((pfit-pver)/np.sqrt(pfit+1+pver+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the difference between the two thinned profiles (green dots) with the theoretically estimated variance in the difference assuming both profiles are observations of Poisson random variables and they are uncorrelated. ($\\sqrt{\\sigma_f^2+\\sigma_g^2}$).  The theoretically estimated standard deviation appears representative of the scatter in the difference signal and the scattered signal appears to be zero mean.\n",
    "\n",
    "The standard deviation normalized mean and standard deviation are reported above these figures.  The mean is near zero and the standard deviation is near one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Optimal Filter\n",
    "The optimal filter kernel in a set is found by applying all the kernels to $\\mathbf{f}$ then scoring the result $\\tilde{\\mathbf{f}}$ against the verification profile $\\mathbf{g}$.  The score is calculated using the inverse log-likelihood\n",
    "$$\\mathcal{E} = \\tilde{\\mathbf{f}} - \\mathbf{g}\\ln \\tilde{\\mathbf{f}} $$\n",
    "The filter that produces the lowest inverse log-likelihood is taken to be the best filter from the set for this particular scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "040adc1c-37dd-46ec-a4d1-5f3554cd1d50"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Filter the raw profile\n",
    "\"\"\"\n",
    "\n",
    "# define range of filters to try\n",
    "filt_size = np.logspace(-1,1,40)\n",
    "\n",
    "# initialize the output arrays\n",
    "inv_ll = np.zeros(filt_size.size)\n",
    "filt_profs = np.zeros((filt_size.size,pfit.size))\n",
    "\n",
    "for ifilt,filter_width in enumerate(filt_size):\n",
    "    # get the convolution kernel\n",
    "    kern = build_Gaussian_kernel(0,filter_width)\n",
    "    \n",
    "    # normalize by the amount of points in a region\n",
    "    # to avoid edge effects\n",
    "    norm = np.ones(pfit.shape)\n",
    "    norm = scipy.signal.convolve2d(norm,kern,mode='same')\n",
    "    # apply the filter and normalize the result\n",
    "    # with the amount of data points contributing\n",
    "    pfilt = scipy.signal.convolve2d(pfit,kern,mode='same')/norm\n",
    "    \n",
    "    pfilt[pfilt==0] = 0.001 # avoid zero values that blow up the log function\n",
    "    \n",
    "    # save the results\n",
    "    filt_profs[ifilt,:] = pfilt\n",
    "    inv_ll[ifilt] = np.nansum(pfilt-pver*np.log(pfilt))\n",
    "\n",
    "# get the index to the solution\n",
    "isol = np.argmin(inv_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "ad9131dc-6483-47ee-9768-fbf810d7aafb"
    }
   },
   "outputs": [],
   "source": [
    "# plot the resulting optimized profile over the raw data\n",
    "# also include an over-filtered example\n",
    "# in a separate plot show the inverse log-likelihood as a function of filter width\n",
    "\n",
    "fig,ax = plt.subplots(2,1,figsize=(5,10))\n",
    "ax[0].plot(pver.flatten(),plidar_range*1e-3,'.',markersize=2,label='raw data')\n",
    "ax[0].plot(filt_profs[isol,:],plidar_range*1e-3,'k--',label='filtered')\n",
    "ax[0].plot(filt_profs[-1,:],plidar_range*1e-3,':',label='over filtered')\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_ylabel('Range [km]')\n",
    "ax[0].set_xlabel('Photon Counts')\n",
    "ax[0].grid(b=True)\n",
    "ax[0].set_ylim([0,12])\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(filt_size*dz,inv_ll*1e-3,'b.')\n",
    "ax[1].plot(filt_size[isol]*dz,inv_ll[isol]*1e-3,'gs')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].grid(b=True)\n",
    "ax[1].set_xlabel('Filter Width [m]')\n",
    "ax[1].set_ylabel('Inverse Log-Likelihood x $10^{-3}$');\n",
    "\n",
    "#ax[1].set_ylim([inv_ll.min()*1.1,0.01*np.median(inv_ll)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plots for publication\n",
    "\"\"\"\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(3.2,3.2))\n",
    "ax.plot(pdemo.flatten(),plidar_range*1e-3,'k.',markersize=2,label='observation')\n",
    "ax.plot(pfit.flatten(),plidar_range*1e-3,'.',markersize=2,alpha=0.5,label='fit data')\n",
    "ax.plot(pver.flatten(),plidar_range*1e-3,'.',markersize=2,alpha=0.5,label='verify data')\n",
    "ax.plot(filt_profs[isol,:],plidar_range*1e-3,'-',linewidth=1,label='filtered')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('Range [km]')\n",
    "ax.set_xlabel('Photon Counts')\n",
    "ax.grid(b=True)\n",
    "ax.set_ylim([0,12])\n",
    "ax.legend(loc=3,prop={'size': 8})\n",
    "plt.savefig('../../plots/Thinned_and_Filt_Data.png',dpi=300,bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(3.1,2.1))\n",
    "ax.plot(filt_size*dz,inv_ll*1e-3,'b.')\n",
    "ax.plot(filt_size[isol]*dz,inv_ll[isol]*1e-3,'gd',alpha=0.5)\n",
    "ax.set_xscale('log')\n",
    "ax.grid(b=True)\n",
    "ax.set_xlabel('Filter Width [m]',fontsize=8)\n",
    "ax.set_ylabel('Inverse Log-Likelihood',fontsize=8)\n",
    "plt.savefig('../../plots/InvLL_Filter.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_size[isol]*dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top figure shows the verification data in blue dots and the optimally filtered data is in the blacked dashed line.  For comparison, an over filtered case is also shown as the orange dotted line.  Overfiltering is more effective as supressing the random errors, but also skews the signal, particularly where clouds are present.\n",
    "\n",
    "The bottom figure shows the inverse log-likelihood for each filter kernel width (defined by the standard deviation of a Gaussian) where the minimum (optimal) value is indicated by the green square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Optimization for Backscatter Ratio\n",
    "Principles described above can be similarly applied to multiple channels in two dimensions.  The example below applies filter optimization to estimating backscatter ratio from an HSRL in low signal-to-noise (SNR) conditions.  There is a significant amount of solar background noise in the molecular and cobmined channels of the lidar, it's operating at low pulse energy and the potassium filter used in the molecular channel is less efficient than its rubidium counterpart.  This is a difficult scene to process.\n",
    "\n",
    "We start by loading both the combined and molecular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_alt = 10e3\n",
    "\n",
    "imax_alt = np.argmin(np.abs(lidar_range-max_alt))\n",
    "\n",
    "hsrl_profs = {}\n",
    "for var in lidar_profile_data:\n",
    "    # assign the demonstration case to its own independent variable\n",
    "    ptemp = profs[var]      # remove bottom three bins from laser \"bang\"\n",
    "    hsrl_profs[var] = {}\n",
    "    hsrl_profs[var]['raw'] = ptemp.copy()\n",
    "    hsrl_profs[var]['bg'] = 0.5*np.mean(hsrl_profs[var]['raw'][:,-100:],axis=1)[:,np.newaxis]\n",
    "    \n",
    "    hsrl_profs[var]['raw']=hsrl_profs[var]['raw'][:,3:imax_alt]\n",
    "    \n",
    "    # Poisson thin the raw photon counts of the profile\n",
    "    hsrl_profs[var]['fit'],hsrl_profs[var]['ver'] = poisson_thin(hsrl_profs[var]['raw'])\n",
    "    \n",
    "hsrl_lidar_range = lidar_range[3:imax_alt]  # create a new range array for the bang-removed profile\n",
    "dz = np.mean(np.diff(hsrl_lidar_range))  # store the range resolution  \n",
    "dt = np.mean(np.diff(time))/np.timedelta64(1, 's')  # store the time resolution  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we would evaluate all combinations of filters in range and time, but its faster and more practical to separate the optimization.  So first we optimize the smoothing in time.  Then optimize in range.  \n",
    "\n",
    "The optimization is applied independently to both molecular and combined channels.\n",
    "\n",
    "In general it is a good idea to remove known structure from the signals before applying smoothing to avoid having it limit the amout of smoothing.  For smoothing in time, we remove the background before applying the kernel, then add it back before evaluating the inverse log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define range of filters to try\n",
    "time_filt_size = np.logspace(-1,3,100)\n",
    "\n",
    "# initialize the output arrays\n",
    "hsrl_inv_ll = {}\n",
    "hsrl_filt_profs = {}\n",
    "for ch in hsrl_profs:\n",
    "    hsrl_inv_ll[ch] = {'time':np.zeros((time_filt_size.size,hsrl_profs[ch]['fit'].shape[1]))}\n",
    "    hsrl_filt_profs[ch] = np.zeros(hsrl_profs[ch]['fit'].shape+(time_filt_size.size,))\n",
    "\n",
    "for ifilt,filter_width in enumerate(time_filt_size):\n",
    "    # get the convolution kernel\n",
    "    kern = build_Gaussian_kernel(filter_width,0)\n",
    "    \n",
    "    # normalize by the amount of points in a region\n",
    "    # to avoid edge effects\n",
    "    norm = np.ones(hsrl_profs['Combined_Counts']['fit'].shape)\n",
    "    norm = scipy.signal.convolve2d(norm,kern,mode='same')\n",
    "    \n",
    "    for ch in hsrl_profs:\n",
    "        # apply the filter and normalize the result\n",
    "        # with the amount of data points contributing\n",
    "        # remove the background to avoid having it limit the smoothing kernel\n",
    "        pfilt = scipy.signal.convolve2d(hsrl_profs[ch]['fit']-hsrl_profs[ch]['bg'],kern,mode='same')/norm\n",
    "        pfilt+=hsrl_profs[ch]['bg']\n",
    "        pfilt[pfilt==0] = 0.001 # avoid zero values that blow up the log function\n",
    "\n",
    "        # save the results\n",
    "        hsrl_filt_profs[ch][:,:,ifilt] = pfilt.copy()\n",
    "        hsrl_inv_ll[ch]['time'][ifilt,:] = np.nansum(pfilt-hsrl_profs[ch]['ver']*np.log(pfilt),axis=0)\n",
    "\n",
    "isol = {}\n",
    "for ch in hsrl_profs:\n",
    "    # get the index to the solution\n",
    "    isol[ch] = {}\n",
    "    isol[ch]['time'] = np.argmin(hsrl_inv_ll[ch]['time'],axis=0)\n",
    "    hsrl_profs[ch]['filtered'] = hsrl_profs[ch]['fit'].copy()\n",
    "    hsrl_profs[ch]['filtered'][:,np.arange(hsrl_filt_profs[ch].shape[1])] = \\\n",
    "                hsrl_filt_profs[ch][:,np.arange(hsrl_filt_profs[ch].shape[1]),isol[ch]['time']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "for ch in hsrl_profs:\n",
    "    plt.plot(time_filt_size[isol[ch]['time']]*dt,hsrl_lidar_range,'.',alpha=0.8,label=ch.replace('_Counts',' '))\n",
    "plt.xlabel('Kernel Width [s]')\n",
    "plt.ylabel('Altitude [m]')\n",
    "plt.xscale('log')\n",
    "plt.grid(b=True)\n",
    "plt.legend(loc=1)\n",
    "plt.savefig('../../plots/Optimized_2D_Filter_Width_Time.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal smoothing kernels in time for each range bin are shown above.  Regions with clouds generally require smaller kernels to preserve the signal structure, where relatively clear altitudes use large kernels to suppress shot noise. \n",
    "\n",
    "We now optimize the range smoothing on the optimized output from time smoothing.  Similar to what we did when removing background, it can help to remove known structure in range before applying the kernel, then reapplying before evaluating the inverse log-likelihood.  We do not do this below, but "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define range of filters to try\n",
    "range_filt_size = np.logspace(-1,1.5,40)\n",
    "\n",
    "# initialize the output arrays\n",
    "hsrl_filt_profs = {}\n",
    "for ch in hsrl_profs:\n",
    "    hsrl_filt_profs[ch] = np.zeros(hsrl_profs[ch]['fit'].shape+(range_filt_size.size,))\n",
    "    hsrl_inv_ll[ch]['range'] = np.zeros((range_filt_size.size,hsrl_profs[ch]['fit'].shape[0]))\n",
    "\n",
    "for ifilt,filter_width in enumerate(range_filt_size):\n",
    "    # get the convolution kernel\n",
    "    kern = build_Gaussian_kernel(0,filter_width)\n",
    "    \n",
    "    # normalize by the amount of points in a region\n",
    "    # to avoid edge effects\n",
    "    norm = np.ones(hsrl_profs['Combined_Counts']['fit'].shape)\n",
    "    norm = scipy.signal.convolve2d(norm,kern,mode='same')\n",
    "    \n",
    "    for ch in hsrl_profs:\n",
    "        # apply the filter and normalize the result\n",
    "        # with the amount of data points contributing\n",
    "        # remove the background to avoid having it limit the smoothing kernel\n",
    "        pfilt = scipy.signal.convolve2d(hsrl_profs[ch]['filtered'],kern,mode='same')/norm\n",
    "        pfilt[pfilt==0] = 0.001 # avoid zero values that blow up the log function\n",
    "\n",
    "        # save the results\n",
    "        hsrl_filt_profs[ch][:,:,ifilt] = pfilt.copy()\n",
    "        hsrl_inv_ll[ch]['range'][ifilt,:] = np.nansum(pfilt-hsrl_profs[ch]['ver']*np.log(pfilt),axis=1)\n",
    "\n",
    "for ch in hsrl_profs:\n",
    "    # get the index to the solution\n",
    "    isol[ch]['range'] = np.argmin(hsrl_inv_ll[ch]['range'],axis=0)\n",
    "    hsrl_profs[ch]['filtered'] = hsrl_profs[ch]['fit'].copy()\n",
    "    hsrl_profs[ch]['filtered'][np.arange(hsrl_filt_profs[ch].shape[0]),:] = \\\n",
    "                hsrl_filt_profs[ch][np.arange(hsrl_filt_profs[ch].shape[0]),:,isol[ch]['range']]\n",
    "    hsrl_profs[ch]['overfilt'] = hsrl_filt_profs[ch][:,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "for ch in hsrl_profs:\n",
    "    plt.plot(time,range_filt_size[isol[ch]['range']]*dz,'.',alpha=0.6,label=ch.replace('_Counts',' '))\n",
    "plt.ylabel('Kernel Width [m]')\n",
    "plt.xlabel('Time [UTC]')\n",
    "plt.yscale('log')\n",
    "plt.grid(b=True)\n",
    "plt.legend()\n",
    "plt.savefig('../../plots/Optimized_2D_Filter_Width_Range.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for ch in hsrl_profs:\n",
    "    plt.figure()\n",
    "    plt.imshow((hsrl_profs[ch]['fit']-hsrl_profs[ch]['bg'])[:,::-1].T,norm=LogNorm())\n",
    "    plt.clim([1,1e3])\n",
    "    plt.figure()\n",
    "    plt.imshow((hsrl_profs[ch]['filtered']-hsrl_profs[ch]['bg'])[:,::-1].T,norm=LogNorm())\n",
    "    plt.clim([1,1e3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above show the background subtracted photon counts of unsmoothed and optimally smoothed profiles for both the combined (top two) and molecular channels (bottom two)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate the backscatter ratio for the difference cases\n",
    "\"\"\"\n",
    "\n",
    "mol_gain = 2.0\n",
    "backscatter_ratio_ver = (hsrl_profs['Combined_Counts']['ver']-hsrl_profs['Combined_Counts']['bg'])/ \\\n",
    "                        (hsrl_profs['Molecular_Counts']['ver']-hsrl_profs['Molecular_Counts']['bg'])/mol_gain\n",
    "backscatter_ratio = (hsrl_profs['Combined_Counts']['fit']-hsrl_profs['Combined_Counts']['bg'])/ \\\n",
    "                        (hsrl_profs['Molecular_Counts']['fit']-hsrl_profs['Molecular_Counts']['bg'])/mol_gain\n",
    "backscatter_ratio_filtered = (hsrl_profs['Combined_Counts']['filtered']-hsrl_profs['Combined_Counts']['bg'])/ \\\n",
    "                        (hsrl_profs['Molecular_Counts']['filtered']-hsrl_profs['Molecular_Counts']['bg'])/mol_gain\n",
    "    \n",
    "backscatter_ratio_overfilt = (hsrl_profs['Combined_Counts']['overfilt']-hsrl_profs['Combined_Counts']['bg'])/ \\\n",
    "                        (hsrl_profs['Molecular_Counts']['overfilt']-hsrl_profs['Molecular_Counts']['bg'])/mol_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(backscatter_ratio[:,::-1].T,norm=LogNorm())\n",
    "plt.clim(1,1e2)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(backscatter_ratio_filtered[:,::-1].T,norm=LogNorm())\n",
    "plt.clim(1,1e2)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(backscatter_ratio_overfilt[:,::-1].T,norm=LogNorm())\n",
    "plt.clim(1,1e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above compare the cases where backscatter ratio is estimated with no smoothing (top), optimized smoothing in time and range (middle) and optimized temporal smoothing but over smoothing in range (bottom).  The top and bottom figures show the extremes of error contributions from stochastic noise (top) and smearing that causes biases (bottom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "ax.plot(backscatter_ratio[itime,:],hsrl_lidar_range,'.-',alpha=0.5,label='unfiltered')\n",
    "ax.plot(backscatter_ratio_filtered[itime,:],hsrl_lidar_range,'.-',label='optimized')\n",
    "ax.plot(backscatter_ratio_overfilt[itime,:],hsrl_lidar_range,'.-',alpha=0.5,label='over filtered')\n",
    "ax.set_ylim([0,10e3])\n",
    "ax.set_xlim([1,100])\n",
    "ax.set_xscale('log')\n",
    "ax.grid(b=True)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figures below are scaled for publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(3.2,3.2))\n",
    "ax.plot(backscatter_ratio[itime,:],hsrl_lidar_range*1e-3,'.-',alpha=0.5,markersize=3,label='unfiltered')\n",
    "ax.plot(backscatter_ratio_filtered[itime,:],hsrl_lidar_range*1e-3,'.-',markersize=3,label='optimized')\n",
    "ax.plot(backscatter_ratio_overfilt[itime,:],hsrl_lidar_range*1e-3,'.-',markersize=3,alpha=0.5,label='over filtered')\n",
    "ax.set_ylim([0,10])\n",
    "ax.set_xlim([1,100])\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('Altitude [km]')\n",
    "ax.set_xlabel('Backscatter Ratio')\n",
    "ax.grid(b=True)\n",
    "ax.legend(prop={'size': 8})\n",
    "plt.savefig('../../plots/Optimized_2D_BSR_Profile.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,1,figsize=(3.2,3.5))\n",
    "for ch in hsrl_profs:\n",
    "    ax[0].plot(time,range_filt_size[isol[ch]['range']]*dz,'.',markersize=2,alpha=0.6,label=ch.replace('_Counts',' '))\n",
    "    ax[1].plot(hsrl_lidar_range,time_filt_size[isol[ch]['time']]*dt,'.',markersize=2,alpha=0.8,label=ch.replace('_Counts',' '))\n",
    "ax[0].set_ylabel('Kernel Width [m]')\n",
    "ax[0].set_xlabel('Time [UTC]')\n",
    "ax[0].tick_params(axis='x', labelsize=7)\n",
    "ax[0].grid(b=True)\n",
    "#ax[0].legend(prop={'size': 8})\n",
    "ax[1].set_ylabel('Kernel Width [s]')\n",
    "ax[1].set_xlabel('Altitude [m]')\n",
    "ax[1].tick_params(axis='x', labelsize=7)\n",
    "ax[1].grid(b=True)\n",
    "ax[1].legend(loc=7,prop={'size': 8})\n",
    "plt.savefig('../../plots/Optimized_2D_Filter_Widths.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax1 = plt.subplot2grid((3, 3), (0, 0), colspan=2,rowspan=2)\n",
    "ax1.pcolor(time,hsrl_lidar_range*1e-3,backscatter_ratio_filtered.T,norm=LogNorm(),vmin=1,vmax=1e2)\n",
    "ax1.set_ylabel('Altitude [km]')\n",
    "ax1.set_title('Backscatter Ratio')\n",
    "ax1.get_xaxis().set_visible(False)\n",
    "ax1.tick_params(axis='y', labelsize=7)\n",
    "\n",
    "ax2 = plt.subplot2grid((3, 3), (2, 0), colspan=2)\n",
    "for ch in hsrl_profs:\n",
    "    ax2.plot(time,range_filt_size[isol[ch]['range']]*dz,'.',markersize=2,alpha=0.5,label=ch.replace('_Counts',' '))\n",
    "ax2.set_xlabel('Time [UTC]')\n",
    "ax2.set_ylabel('Range Resolution [m]')\n",
    "ax2.grid(b=True,which='both',axis='y')\n",
    "ax2.grid(b=True,which='major',axis='x')\n",
    "ax2.tick_params(axis='x', labelsize=7)\n",
    "ax2.tick_params(axis='y', labelsize=7)\n",
    "\n",
    "\n",
    "ax3 = plt.subplot2grid((3, 3), (0, 2), rowspan=2)\n",
    "for ch in hsrl_profs:\n",
    "    ax3.plot(time_filt_size[isol[ch]['time']]*dt,hsrl_lidar_range,'.',markersize=2,alpha=0.5,label=ch.replace('_Counts',' '))\n",
    "ax3.set_xlabel('Time Resolution [s]')\n",
    "labels = [item.get_text() for item in ax3.get_yticklabels()]\n",
    "empty_string_labels = ['']*len(labels)\n",
    "ax3.set_yticklabels(empty_string_labels)\n",
    "\n",
    "#ax3.get_yaxis().set_visible(False)\n",
    "ax3.grid(b=True,which='both',axis='x')\n",
    "ax3.grid(b=True,which='major',axis='y')\n",
    "\n",
    "\n",
    "plt.savefig('../../plots/Optimized_2D_Filter_w_BSR.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "02b24369-ab1a-4f0b-9d45-538d8c36357f",
    "theme": {}
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
